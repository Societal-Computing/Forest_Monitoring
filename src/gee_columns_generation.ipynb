{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYljfrMBwslv"
   },
   "source": [
    "**Introduction**\n",
    "Generating the Extra Columns wiith earth Engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "VFCs7UizgEfC",
    "outputId": "2b6771b4-2ad6-4d89-b5b3-e5b45c78c620"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angela/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import geemap\n",
    "import ee\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# from google.colab import auth , drive\n",
    "from helper_functions import calculate_area, calculate_built_area, calculate_road_length, calculate_forest_loss,process_month,extract_polygons\n",
    "from helper_functions import calculate_elevation_and_slope, get_savi_for_month, get_ndvi_for_month,get_ndre_for_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "lYlQ3kCMXlXV",
    "outputId": "6e070639-84b4-4dd6-cffc-6f77604517ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 1229175 entries, 0 to 1229174\n",
      "Data columns (total 17 columns):\n",
      " #   Column                        Non-Null Count    Dtype   \n",
      "---  ------                        --------------    -----   \n",
      " 0   site_id_created               1229175 non-null  int64   \n",
      " 1   project_id_reported           1229175 non-null  object  \n",
      " 2   site_id_reported              1229175 non-null  object  \n",
      " 3   site_description_reported     1696 non-null     object  \n",
      " 4   site_sqkm                     1229175 non-null  float64 \n",
      " 5   trees_planted_reported        4349 non-null     float64 \n",
      " 6   country                       5030 non-null     object  \n",
      " 7   project_description_reported  1228611 non-null  object  \n",
      " 8   planting_date_reported        4821 non-null     float64 \n",
      " 9   survival_rate_reported        2514 non-null     float64 \n",
      " 10  host_name                     1229175 non-null  object  \n",
      " 11  url                           1229175 non-null  object  \n",
      " 12  species_count_reported        0 non-null        float64 \n",
      " 13  species_planted_reported      171 non-null      object  \n",
      " 14  geometry                      1229174 non-null  geometry\n",
      " 15  Creator                       94 non-null       object  \n",
      " 16  project_id_created            1229175 non-null  int64   \n",
      "dtypes: float64(5), geometry(1), int64(2), object(9)\n",
      "memory usage: 159.4+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = gpd.read_parquet(\"../midsave/consolidated_reforestation_projects.parquet\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf = df.loc[:,['site_id_created', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf.dropna(subset=['geometry'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "print(gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id_created</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-49.95883 -9.35107, -49.95976 -9.351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-43.4725 -22.48945, -43.47236 -22.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((-43.462 -22.4779, -43.46583 -22.4875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((-43.46833 -22.4919, -43.46834 -22.49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((-2.01902 8.21743, -2.02027 8.2264, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>426</td>\n",
       "      <td>POLYGON ((-64.25714 46.00024, -64.25727 46.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>427</td>\n",
       "      <td>POLYGON ((-64.25637 46.00161, -64.2567 46.0008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>428</td>\n",
       "      <td>POLYGON ((-64.27435 46.02906, -64.27412 46.029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>429</td>\n",
       "      <td>POLYGON ((-64.27349 46.02907, -64.27274 46.029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>430</td>\n",
       "      <td>POLYGON ((-64.26996 46.0296, -64.2703 46.02904...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>430 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     site_id_created                                           geometry\n",
       "0                  0  POLYGON ((-49.95883 -9.35107, -49.95976 -9.351...\n",
       "1                  1  POLYGON ((-43.4725 -22.48945, -43.47236 -22.48...\n",
       "2                  2  POLYGON ((-43.462 -22.4779, -43.46583 -22.4875...\n",
       "3                  3  POLYGON ((-43.46833 -22.4919, -43.46834 -22.49...\n",
       "4                  4  POLYGON ((-2.01902 8.21743, -2.02027 8.2264, -...\n",
       "..               ...                                                ...\n",
       "426              426  POLYGON ((-64.25714 46.00024, -64.25727 46.000...\n",
       "427              427  POLYGON ((-64.25637 46.00161, -64.2567 46.0008...\n",
       "428              428  POLYGON ((-64.27435 46.02906, -64.27412 46.029...\n",
       "429              429  POLYGON ((-64.27349 46.02907, -64.27274 46.029...\n",
       "430              430  POLYGON ((-64.26996 46.0296, -64.2703 46.02904...\n",
       "\n",
       "[430 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf[:430]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a subsample for testing code ! Delete when done !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty GeoDataFrame\n",
      "Columns: [geometry, geometry_type]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "gdf['geometry'] = gdf['geometry'].apply(extract_polygons)\n",
    "\n",
    "\n",
    "gdf['geometry_type'] = gdf['geometry'].apply(lambda geom: geom.geom_type if geom else None)\n",
    "\n",
    "\n",
    "geometry_collection_gdf = gdf[gdf['geometry_type'] == 'GeometryCollection']\n",
    "print(geometry_collection_gdf[['geometry', 'geometry_type']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate with Google Earth Engine\n",
    "Need to log in to EarthEngine (ee.Authenticate()), create a project and then initialize this project via ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "vXiObiAgxscR",
    "outputId": "afc4ca4e-e2be-403b-8fc8-4b2ff6cd57df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ee.Initialize(project='spring-idiom-398208')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnbpOL6EbmK4"
   },
   "source": [
    "### Calculating Tree Cover\n",
    "- tree_cover_area_2000\n",
    "- tree_cover_area_2005\n",
    "- tree_cover_area_2010\n",
    "- tree_cover_area_2015\n",
    "- tree_cover_area_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50\n",
    "chunks = [gdf[i:i + chunk_size] for i in range(0, gdf.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLAD Landcover (https://glad.umd.edu/dataset/GLCLUC2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmask = ee.Image(\"projects/glad/OceanMask\").lte(1)\n",
    "landCover = ee.Image('projects/glad/GLCLU2020/v2/LCLUC_2020').updateMask(landmask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking land cover image to only include class codes of interest for tree cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classCodes = [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,\n",
    "              125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]\n",
    "maskedLandCover = landCover.remap(classCodes, classCodes, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating area for each class list in the codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../midsave/tree_cover.csv'\n",
    "tree_cover = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "bwlZmsIgbaFh",
    "outputId": "3c26595c-6026-40ab-938a-f843bab0f63e"
   },
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    # Converting GeoDataFrame chunk to GeoJSON\n",
    "    gdf_json_chunk = chunk.__geo_interface__\n",
    "    \n",
    "    valid_features = []\n",
    "    for feature in gdf_json_chunk['features']:\n",
    "        try:\n",
    "            # Attempt to convert each feature individually\n",
    "            ee_feature = geemap.geojson_to_ee(feature)\n",
    "            valid_features.append(ee_feature)\n",
    "        except Exception as geom_error:\n",
    "            print(f\"Problematic geometry in chunk {i + 1}: {feature['geometry']} - {geom_error}\")\n",
    "\n",
    "    if not valid_features:\n",
    "        print(f\"No valid features in chunk {i + 1}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Creating a FeatureCollection from valid features\n",
    "        fc_chunk = ee.FeatureCollection(valid_features)\n",
    "        \n",
    "        # Mapping the area calculation function over the FeatureCollection\n",
    "        area_results_chunk = fc_chunk.map(lambda feature: calculate_area(feature, classCodes, maskedLandCover))\n",
    "        temp_chunk_df = pd.DataFrame([feature['properties'] for feature in area_results_chunk.getInfo()['features']])\n",
    "        temp_chunk_df.rename(columns={'cover_area_2020': 'tree_cover_area_2020'}, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Appending to combined DataFrame\n",
    "    tree_cover = pd.concat([tree_cover, temp_chunk_df], ignore_index=True)\n",
    "\n",
    "    # Saving combined results to the output CSV\n",
    "    tree_cover.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"All chunks processed, combined results saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9bzuuIzb7E-"
   },
   "source": [
    "### Calculating other land cover\n",
    "- permanent_water\n",
    "- short_vegetation_after_tree_loss\n",
    "- cropland_loss_to_tree\n",
    "- cropland_gain_from_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classCodes = [208, 240, 248, 245]\n",
    "classesOfInterest = [\"permanent_water\", \"short_vegetation_after_tree_loss\", \"cropland_loss_to_tree\", \"cropland_gain_from_trees\"]\n",
    "maskedLandCover = landCover.remap(classCodes, classCodes, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../midsave/other_land_cover.csv'\n",
    "other_land_cover = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    # Converting GeoDataFrame chunk to GeoJSON and then Earth Engine FeatureCollection\n",
    "    gdf_json_chunk = chunk.__geo_interface__\n",
    "    \n",
    "    try:\n",
    "        # Converting to Earth Engine FeatureCollection\n",
    "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Mapping the area calculation function over the FeatureCollection\n",
    "        area_results_chunk = fc_chunk.map(lambda feature: calculate_area(feature, classCodes, maskedLandCover))\n",
    "        if area_results_chunk:\n",
    "            temp_chunk_df = pd.DataFrame([feature['properties'] for feature in area_results_chunk.getInfo()['features']])\n",
    "            temp_chunk_df.rename(columns={'cover_area_2020': 'other_land_cover_area_2020'}, inplace=True)\n",
    "        else:\n",
    "            temp_chunk_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    other_land_cover = pd.concat([other_land_cover, temp_chunk_df], ignore_index=True)\n",
    "\n",
    "    other_land_cover.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"All chunks processed, combined results saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGtZpPxwuY5x"
   },
   "source": [
    "### Calculate built-area shares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Earth Engine built area image for 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builtImage = ee.Image(\"JRC/GHSL/P2023A/GHS_BUILT_C/2018\").select('built_characteristics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../midsave/built_area_cover.csv'\n",
    "built_area_cover = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing built area for chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    # Converting GeoDataFrame chunk to GeoJSON and then Earth Engine FeatureCollection\n",
    "    gdf_json_chunk = chunk.__geo_interface__\n",
    "\n",
    "    try:\n",
    "        # Converting the GeoJSON chunk to Earth Engine FeatureCollection\n",
    "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Mapping the built area calculation function over the FeatureCollection\n",
    "        built_area_results_chunk = fc_chunk.map(lambda feature: calculate_built_area(feature, builtImage))\n",
    "        if built_area_results_chunk:\n",
    "            temp_chunk_df = pd.DataFrame([feature['properties'] for feature in built_area_results_chunk.getInfo()['features']])\n",
    "        else:\n",
    "            temp_chunk_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    built_area_cover = pd.concat([built_area_cover, temp_chunk_df], ignore_index=True)\n",
    "\n",
    "    built_area_cover.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"All chunks processed, built area results saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-oBg_Hhuhcy"
   },
   "source": [
    "### Calculating road network cover\n",
    "- total_road_length_km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging GEE road datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roadsAfrica = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-africa')\n",
    "roadsAmericas = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-americas')\n",
    "roadsAsia = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-asia')\n",
    "roadsEurope = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-europe')\n",
    "roadsOceaniaEast = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-oceania-east')\n",
    "roadsOceaniaWest = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-oceania-west')\n",
    "\n",
    "roads = roadsAfrica.merge(roadsAmericas).merge(roadsAsia).merge(roadsEurope).merge(roadsOceaniaEast).merge(roadsOceaniaWest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../midsave/road_length.csv'\n",
    "road_length = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing road length for chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    gdf_json_chunk = chunk.__geo_interface__\n",
    "\n",
    "    try:\n",
    "        # Converting the GeoJSON chunk to Earth Engine FeatureCollection\n",
    "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Mapping the road length calculation function over the FeatureCollection\n",
    "        road_length_results_chunk = fc_chunk.map(lambda feature: calculate_road_length(feature, roads))\n",
    "        if road_length_results_chunk:\n",
    "            temp_chunk_df = pd.DataFrame([feature['properties'] for feature in road_length_results_chunk.getInfo()['features']])\n",
    "        else:\n",
    "            temp_chunk_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    road_length = pd.concat([road_length, temp_chunk_df], ignore_index=True)\n",
    "\n",
    "    road_length.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"All chunks processed, road length results saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCScIwE2vgn_"
   },
   "source": [
    "### Calculating forest loss\n",
    "- loss_pre_5\n",
    "- loss_post_3\n",
    "- loss_post_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Global Forest Change 2023 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfc2017 = ee.Image('UMD/hansen/global_forest_change_2023_v1_11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../midsave/forest_loss.csv'\n",
    "forest_loss = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing forest loss for chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    # Converting the current GeoDataFrame chunk to GeoJSON\n",
    "    gdf_json_chunk = chunk.__geo_interface__\n",
    "\n",
    "    try:\n",
    "        # Converting GeoJSON chunk to Earth Engine FeatureCollection\n",
    "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Mapping the forest loss calculation function over the FeatureCollection\n",
    "        loss_results_chunk = fc_chunk.map(lambda feature: calculate_forest_loss(feature, gfc2017))\n",
    "        if loss_results_chunk:\n",
    "            temp_chunk_df = pd.DataFrame([feature['properties'] for feature in loss_results_chunk.getInfo()['features']])\n",
    "        else:\n",
    "            temp_chunk_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    forest_loss = pd.concat([forest_loss, temp_chunk_df], ignore_index=True)\n",
    "\n",
    "print(\"All chunks processed, forest loss results saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWy6FPvg_Tdp"
   },
   "source": [
    "#### Generating the loss columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = (forest_loss['groups']\n",
    "               .explode()\n",
    "               .dropna()\n",
    "               .reset_index()\n",
    "               .rename(columns = {'index':'site_id_created'})\n",
    "               .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = (pd.json_normalize(temp['groups']).reset_index()\n",
    "               .merge(temp[['index','site_id_created']], on = 'index', how = 'left').drop(columns = ['index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['year'] = 2000 + temp['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_loss = temp.pivot(index=['site_id_created'], columns='year', values='sum')\n",
    "forest_loss.columns = [col for col in forest_loss.columns]\n",
    "#forest_loss.columns = [f'forest_loss_{col}' for col in forest_loss.columns]\n",
    "forest_loss.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add planting date information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_loss = forest_loss.merge(df[['site_id_created','planting_date_reported']], on = 'site_id_created', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-g1u3I6AOfn"
   },
   "outputs": [],
   "source": [
    "def calculate_loss_pre_5(row):\n",
    "    try:\n",
    "        years = [int(row['planting_date_reported'] - i) for i in range(1, 6)]\n",
    "        losses = [row[year] for year in years if year in forest_loss.columns]\n",
    "        return np.nanmean(losses) if losses else np.nan\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "def calculate_loss_post_3(row):\n",
    "    try:\n",
    "        years = [int(row['planting_date_reported'] + i) for i in range(1, 4)]\n",
    "        losses = [row[year] for year in years if year in forest_loss.columns]\n",
    "        return np.nanmean(losses) if losses else np.nan\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "def calculate_loss_post_5(row):\n",
    "    try:\n",
    "        years = [int(row['planting_date_reported'] + i) for i in range(1, 6)]\n",
    "        losses = [row[year] for year in years if year in forest_loss.columns]\n",
    "        return np.nanmean(losses) if losses else np.nan\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "forest_loss['loss_pre_5'] = forest_loss.apply(calculate_loss_pre_5, axis=1)\n",
    "forest_loss['loss_post_3'] = forest_loss.apply(calculate_loss_post_3, axis=1)\n",
    "forest_loss['loss_post_5'] = forest_loss.apply(calculate_loss_post_5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_loss.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "un1PPqdmATx_",
    "outputId": "75c811ee-6ed2-411a-f6b6-abcef49d09f2"
   },
   "outputs": [],
   "source": [
    "forest_loss.drop(columns = [year for year in range(2000, 2024)] + ['planting_date_reported'], errors = 'ignore', inplace = True)\n",
    "forest_loss.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_loss.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating elevation and slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Digital Elevations Model (DEM) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ee.Image('USGS/SRTMGL1_003')\n",
    "elevation = dataset.select('elevation')\n",
    "slope = ee.Terrain.slope(elevation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../midsave/elevation_slope.csv'\n",
    "elevation_slope = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    # Converting GeoDataFrame chunk to GeoJSON and then to Earth Engine FeatureCollection\n",
    "    gdf_json_chunk = chunk.__geo_interface__\n",
    "\n",
    "    try:\n",
    "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        results_chunk = fc_chunk.map(lambda feature: calculate_elevation_and_slope(feature, elevation, slope))\n",
    "        if results_chunk:\n",
    "            temp_chunk_df = pd.DataFrame([feature['properties'] for feature in results_chunk.getInfo()['features']])\n",
    "        else:\n",
    "            temp_chunk_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Appending to combined DataFrame\n",
    "    elevation_slope = pd.concat([elevation_slope, temp_chunk_df], ignore_index=True)\n",
    "\n",
    "    elevation_slope.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"All chunks processed, combined results saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUqO4WBwLIS"
   },
   "source": [
    "### Calculate NDVI per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Sentinel-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/2459 for month 1...\n",
      "Processing chunk 1/2459 for month 2...\n",
      "Processing chunk 1/2459 for month 3...\n",
      "Processing chunk 1/2459 for month 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "chunk_size = 500\n",
    "chunks = [gdf[i:i + chunk_size] for i in range(0, gdf.shape[0], chunk_size)]\n",
    "S2 = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n",
    "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n",
    "output_csv_path = '../midsave/ndvi_top3.csv'\n",
    "ndvi_monthly = pd.DataFrame()\n",
    "months = list(range(1, 13))\n",
    "\n",
    "while months:\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system\n",
    "            futures = {executor.submit(process_month, month, chunks, S2): month for month in months}\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(months)):\n",
    "                month_result = future.result()\n",
    "                ndvi_monthly = pd.concat([ndvi_monthly, month_result], ignore_index=True)\n",
    "        \n",
    "        # If all months are processed successfully, break the loop\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if 'month' in ndvi_monthly.columns:\n",
    "            months = list(set(months) - set(ndvi_monthly.month.unique().tolist()))\n",
    "        else:\n",
    "            months = list(set(months))\n",
    "        continue\n",
    "\n",
    "# Saving the results to a CSV file\n",
    "ndvi_monthly.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "while months:\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = {executor.submit(process_month, month, chunks, S2): month for month in months}\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(months)):\n",
    "                month_result = future.result()\n",
    "                ndvi_monthly = pd.concat([ndvi_monthly, month_result], ignore_index=True)\n",
    "        \n",
    "        # If all months are processed successfully, break the loop\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if 'month' in ndvi_monthly.columns:\n",
    "            months = list(set(months) - set(ndvi_monthly.month.unique().tolist()))\n",
    "        else:\n",
    "            months = list(set(months))\n",
    "        continue\n",
    "\n",
    "# Save the results to a CSV file\n",
    "ndvi_monthly.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select three months with the highest NDVI values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_top3 = (ndvi_monthly\n",
    "             .groupby('site_id_created', group_keys=False)[['site_id_created', 'month', 'mean']]\n",
    "             .apply(lambda x: x.nlargest(3, 'mean'))\n",
    "             .rename(columns = {'mean':'ndvi_monthly_mean'})\n",
    "             .reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_top3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V6MQ6bIDJV-"
   },
   "outputs": [],
   "source": [
    "ndvi_top3.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVI(Soil Adjust Vegetation Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before SAVI ensure Top_Three_NDVI_Months and planting dates are added to df data (After creating the column Top_Three_Ndvi_months rerun cell 5 where chunks are processed to update and include the column in the chunk before running the Shadow index cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_si = (gdf\n",
    "       .merge(df[['site_id_created', 'planting_date_reported']], on = 'site_id_created', how = 'left')\n",
    "       .merge(ndvi_top3[['site_id_created', 'month', 'ndvi_monthly_mean']], on = 'site_id_created', how = 'left')\n",
    "       .dropna()\n",
    "       .reset_index(drop = True))\n",
    "gdf_si['planting_date_reported'] = gdf_si['planting_date_reported'].astype(int)\n",
    "gdf_si['month'] = gdf_si['month'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10\n",
    "chunks = [gdf_si[i:i + chunk_size] for i in range(0, gdf_si.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../midsave/savi_index.csv'\n",
    "savi_index = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    gdf_json_chunk = chunk.__geo_interface__\n",
    "\n",
    "    try:\n",
    "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        savi_index_chunk = fc_chunk.map(lambda feature: get_savi_for_month(feature, S2))\n",
    "        if savi_index_chunk:\n",
    "            temp_chunk_df = pd.DataFrame([feature['properties'] for feature in savi_index_chunk.getInfo()['features']])\n",
    "        else:\n",
    "            temp_chunk_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Appending to combined DataFrame\n",
    "    savi_index = pd.concat([savi_index, temp_chunk_df], ignore_index=True)\n",
    "\n",
    "savi_index_grouped = savi_index.groupby(['site_id_created'])['savi_index'].mean().reset_index()\n",
    "savi_index_grouped.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"All chunks processed, combined results saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savi_index_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDVI\n",
    "- atplanting\n",
    "- 1 year after planting\n",
    "- 2 years after planting\n",
    "- 5 years after planting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../midsave/ndvi.csv'\n",
    "ndvi = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    gdf_json_chunk = chunk.__geo_interface__\n",
    "\n",
    "    try:\n",
    "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        ndvi_chunk = fc_chunk.map(lambda feature: get_ndvi_for_month(feature, S2))\n",
    "        if ndvi_chunk:\n",
    "            temp_chunk_df = pd.DataFrame([feature['properties'] for feature in ndvi_chunk.getInfo()['features']])\n",
    "        else:\n",
    "            temp_chunk_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Appending to combined DataFrame\n",
    "    ndvi = pd.concat([ndvi, temp_chunk_df], ignore_index=True)\n",
    "\n",
    "ndvi_grouped = ndvi.groupby(['site_id_created'])['ndvi'].mean().reset_index()\n",
    "ndvi_grouped.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"All chunks processed, combined results saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_monthly = pd.read_csv('../midsave/ndvi.csv')\n",
    "\n",
    "ndvi_monthly.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../midsave/ndre.csv'\n",
    "ndre = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "    gdf_json_chunk = chunk.__geo_interface__\n",
    "\n",
    "    try:\n",
    "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        ndre_chunk = fc_chunk.map(lambda feature: get_ndre_for_month(feature, S2))\n",
    "        if ndre_chunk:\n",
    "            temp_chunk_df = pd.DataFrame([feature['properties'] for feature in ndre_chunk.getInfo()['features']])\n",
    "        else:\n",
    "            temp_chunk_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Appending to combined DataFrame\n",
    "    ndre = pd.concat([ndre, temp_chunk_df], ignore_index=True)\n",
    "\n",
    "ndre_grouped = ndre.groupby(['site_id_created'])['ndre'].mean().reset_index()\n",
    "ndre_grouped.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"All chunks processed, combined results saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_monthly = pd.read_csv('../midsave/ndre.csv')\n",
    "\n",
    "ndvi_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = (df\n",
    "               .merge(tree_cover, on = 'site_id_created', how = 'left')\n",
    "               .merge(other_land_cover, on = 'site_id_created', how = 'left')\n",
    "               .merge(built_area_cover, on = 'site_id_created', how = 'left')\n",
    "               .merge(road_length, on = 'site_id_created', how = 'left')\n",
    "               .merge(forest_loss, on = 'site_id_created', how = 'left')\n",
    "               .merge(elevation_slope, on = 'site_id_created', how = 'left')\n",
    "               .merge(ndvi_top3, on = 'site_id_created', how = 'left')\n",
    "               .merge(savi_index_grouped, on = 'site_id_created', how = 'left')\n",
    "               .merge(ndvi_grouped, on = 'site_id_created', how = 'left')\n",
    "               .merge(ndre_grouped, on = 'site_id_created', how = 'left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.info()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
