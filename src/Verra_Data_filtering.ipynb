{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting polygons data from kml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykml import parser\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Extracting data from .kml file downloaded from https://registry.verra.org/app/search/VCS\n",
    "with open('/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/kml_files/03-Dingxi_Project boundary.kml', 'r') as kml_file:\n",
    "    kml_content = parser.parse(kml_file).getroot().Document\n",
    "\n",
    "\n",
    "names = []\n",
    "descriptions = []\n",
    "coordinates = []\n",
    "\n",
    "\n",
    "def parse_coordinates(coordinate_string):\n",
    "    coordinate_list = coordinate_string.strip().split()\n",
    "    return [(float(coordinate.split(',')[1]), float(coordinate.split(',')[0])) for coordinate in coordinate_list]\n",
    "\n",
    "def parse_description(description):\n",
    "    if description is None:\n",
    "        return None\n",
    "    soup = BeautifulSoup(description, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    data = {}\n",
    "    for row in table.find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) > 1:\n",
    "            key = cells[0].text.strip().replace(':', '')\n",
    "            value = cells[1].text.strip()\n",
    "            data[key] = value\n",
    "    return data\n",
    "\n",
    "\n",
    "for folder in kml_content.Folder:\n",
    "\n",
    "    for placemark in folder.Placemark:\n",
    "        name = placemark.name.text if hasattr(placemark, 'name') else None\n",
    "        description = parse_description(placemark.description.text) if hasattr(placemark, 'description') and placemark.description.text is not None else None\n",
    "        if hasattr(placemark, 'Point'):\n",
    "            coordinate = parse_coordinates(placemark.Point.coordinates.text)\n",
    "        elif hasattr(placemark, 'LineString'):\n",
    "            coordinate = parse_coordinates(placemark.LineString.coordinates.text)\n",
    "        else:\n",
    "            coordinate = None\n",
    "        names.append(name)\n",
    "        descriptions.append(description)\n",
    "        coordinates.append(coordinate)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Description': descriptions,\n",
    "    'Coordinates': coordinates\n",
    "})\n",
    "\n",
    "\n",
    "df.to_csv('/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/kml_files/03-Dingxi_Project boundary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Concatinating Polygons with trees and area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_With_tree_number'  \n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "\n",
    "for filename in all_files:\n",
    "\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "concatenated_df_TIST = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "concatenated_df_TIST.to_csv('/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_With_tree_number/concatenated_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = 'TIST_Projects/KML_737_02NOV2011.csv'\n",
    "\n",
    "\n",
    "concatenated_df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "concatenated_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING_TIST_Data _clening\n",
    "\n",
    "df = concatenated_df\n",
    "\n",
    "\n",
    "cleaned_data = []\n",
    "\n",
    "\n",
    "for i in range(0, len(df) - 1, 2):\n",
    "  \n",
    "    name = df.iloc[i]['Name']\n",
    "    description = df.iloc[i]['Description']\n",
    "    \n",
    " \n",
    "    coordinates = df.iloc[i + 1]['Coordinates']\n",
    "    \n",
    "    cleaned_data.append([name, description, coordinates])\n",
    "\n",
    "\n",
    "cleaned_df = pd.DataFrame(cleaned_data, columns=['Name', 'Description', 'Coordinates'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cleaned_df.info()\n",
    "\n",
    "cleaned_df.to_csv(\"KML_737_02NOV2011.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_With_tree_number/concatenated_file.csv'\n",
    "\n",
    "\n",
    "concatenated_df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "concatenated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.drop(columns=['Unnamed: 3'], inplace=True)\n",
    "concatenated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def expand_description(description_series):\n",
    "    \n",
    "    expanded_data = []\n",
    "    \n",
    "  \n",
    "    for item in description_series:\n",
    "      \n",
    "        if isinstance(item, str):\n",
    "            item = literal_eval(item)\n",
    "        expanded_data.append(item)\n",
    "    \n",
    "  \n",
    "    return pd.DataFrame(expanded_data)\n",
    "\n",
    "\n",
    "expanded_description_df = expand_description(concatenated_df['Description'])\n",
    "\n",
    "\n",
    "TIST_Cleaned_df = pd.concat([concatenated_df.drop('Description', axis=1).reset_index(drop=True), expanded_description_df], axis=1)\n",
    "TIST_Cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIST_Cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TIST_Cleaned_df['Trees'] = pd.to_numeric(TIST_Cleaned_df['Trees'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "\n",
    "trees_sum = TIST_Cleaned_df['Trees'].sum()\n",
    "\n",
    "\n",
    "print(trees_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TIST_Cleaned_df['Grove Area'] = TIST_Cleaned_df['Grove Area'].str.replace(' Ha', '').str.replace(',', '').replace('', np.nan)\n",
    "\n",
    "\n",
    "TIST_Cleaned_df['Grove Area'] = pd.to_numeric(TIST_Cleaned_df['Grove Area'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_area = TIST_Cleaned_df['Grove Area'].sum()\n",
    "\n",
    "\n",
    "print(total_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIST_Cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIST_Cleaned_df['Coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import simplekml\n",
    "\n",
    "def row_to_kml(row, index, folder_path):\n",
    "    kml = simplekml.Kml()\n",
    "    polygon = kml.newpolygon(name=f\"Polygon{index}\")\n",
    "\n",
    "    coordinates = eval(row['Coordinates'])\n",
    "    polygon.outerboundaryis = coordinates\n",
    "    kml.save(f\"{folder_path}/Polygon{index}.kml\")\n",
    "\n",
    "#\n",
    "df = TIST_Cleaned_df\n",
    "\n",
    "folder_path = \"kml_files/Individual_Polygons_kml\"\n",
    "for index, row in df.iterrows():\n",
    "    row_to_kml(row, index, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import simplekml\n",
    "import ast   \n",
    "\n",
    "def row_to_kml(row, index, folder_path):\n",
    "    kml = simplekml.Kml()\n",
    "    polygon = kml.newpolygon(name=f\"Polygon{index}\")\n",
    " \n",
    "    coordinates_str = row['Coordinates']\n",
    "    coordinates_list = ast.literal_eval(coordinates_str)\n",
    "    \n",
    "    \n",
    "    formatted_coordinates = [(lon, lat, 0.0) for lon, lat in coordinates_list]\n",
    "    \n",
    "    polygon.outerboundaryis = formatted_coordinates\n",
    "    \n",
    "    kml.save(f\"{folder_path}/Polygon{index}.kml\")\n",
    "\n",
    " \n",
    "df = TIST_Cleaned_df\n",
    "\n",
    "folder_path = \"kml_files/Individual_Polygons_kml\"\n",
    "for index, row in df.iterrows():\n",
    "    row_to_kml(row, index, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import simplekml\n",
    "import ast   \n",
    "\n",
    "def row_to_kml(row, index, folder_path):\n",
    "    kml = simplekml.Kml()\n",
    "    polygon = kml.newpolygon(name=f\"Polygon{index}\")\n",
    "    \n",
    " \n",
    "    coordinates_str = row['Coordinates']\n",
    "    coordinates_list = ast.literal_eval(coordinates_str)\n",
    "  \n",
    "    formatted_coordinates = [(lat, lon, 0.0) for lon, lat in coordinates_list]\n",
    "    \n",
    "    polygon.outerboundaryis = formatted_coordinates\n",
    "        # Set polygon style\n",
    "    polygon.style.polystyle.color = simplekml.Color.changealphaint(10, simplekml.Color.white)  \n",
    "    polygon.style.linestyle.width = 5  \n",
    "    polygon.style.linestyle.color = simplekml.Color.blue   \n",
    "    file_name = f\"Polygon{index}\"\n",
    "    kml.save(f\"{folder_path}/Polygon{index}.kml\")\n",
    "    return file_name\n",
    "\n",
    " \n",
    "df = TIST_Cleaned_df\n",
    "\n",
    "folder_path = \"kml_files/Individual_Polygons_kml\"\n",
    "kml_file_names = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    file_name = row_to_kml(row, index, folder_path)\n",
    "    kml_file_names.append(file_name)\n",
    " \n",
    "df['kml_file_name'] = kml_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_With_tree_number/TIST_Meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "df['Tract Date'] = pd.to_datetime(df['Tract Date'])\n",
    "\n",
    " \n",
    "df['Year'] = df['Tract Date'].dt.year\n",
    "\n",
    " \n",
    "year_counts = df['Year'].value_counts().sort_index()\n",
    "\n",
    " \n",
    "print(\"Yearly Tract Counts:\")\n",
    "print(year_counts)\n",
    " \n",
    "plt.figure(figsize=(10, 6))\n",
    "year_counts.plot(kind='bar')\n",
    "plt.title('Count of Tracts per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_With_tree_number/TIST_Meta.csv'\n",
    "meta= pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "meta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_With_tree_number/concatenated_file.csv'\n",
    "\n",
    "\n",
    "concatenated_df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "concatenated_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatinating Data for polygons without Tree information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    " \n",
    "path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_without_trees'  \n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "\n",
    "for filename in all_files:\n",
    "\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "concatenated_df_TIST = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "concatenated_df_TIST.to_csv('/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_without_trees/concatenated_wt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_without_trees/concatenated_wt.csv'\n",
    "\n",
    "\n",
    "concatenated_df_TIST= pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "concatenated_df_TIST.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "def latlong_to_utm(coords):\n",
    "    \"\"\"Convert lat-long coordinates to UTM.\"\"\"\n",
    "    \n",
    "    proj_latlong = Proj(proj='latlong', datum='WGS84')\n",
    "    proj_utm = Proj(proj='utm', zone=33, datum='WGS84')\n",
    "     \n",
    "    utm_coords = [transform(proj_latlong, proj_utm, coord[1], coord[0]) for coord in coords]\n",
    "    return utm_coords\n",
    "\n",
    "def parse_polygon_coords(polygon_str):\n",
    "    \"\"\"Parse the polygon coordinates from a string.\"\"\"\n",
    "    coords = re.findall(r\"\\(([^)]+)\\)\", polygon_str)\n",
    "    return [tuple(map(float, coord.split(','))) for coord in coords]\n",
    "\n",
    "def calculate_polygon_area(coords):\n",
    "    \"\"\"Calculate the area of a polygon in square meters using the Shoelace formula.\"\"\"\n",
    "     \n",
    "    utm_coords = latlong_to_utm(coords)\n",
    "    n = len(utm_coords)\n",
    "    area = 0.0\n",
    "    for i in range(n):\n",
    "        j = (i + 1) % n\n",
    "        area += utm_coords[i][0] * utm_coords[j][1]\n",
    "        area -= utm_coords[j][0] * utm_coords[i][1]\n",
    "    area = abs(area) / 2.0\n",
    "    return area\n",
    "\n",
    " \n",
    "concatenated_df_TIST['Area_m2'] = concatenated_df_TIST['Polygon_Geometry'].apply(\n",
    "    lambda x: calculate_polygon_area(parse_polygon_coords(x)) if isinstance(x, str) else None\n",
    ")\n",
    "\n",
    "concatenated_df_TIST.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "concatenated_df_TIST['Area_ha'] = concatenated_df_TIST['Polygon_Geometry'].apply(\n",
    "    lambda x: calculate_polygon_area(parse_polygon_coords(x)) / 10000 if isinstance(x, str) else None\n",
    ")\n",
    "\n",
    "concatenated_df_TIST.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area_ha = concatenated_df_TIST['Area_ha'].sum()\n",
    "print(f\"Total area covered: {total_area_ha} hectares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komaza/Coastal Kenya Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = 'Komaza_Carbon_Project_Farms.csv'\n",
    "\n",
    "\n",
    "komaza_df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "komaza_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Seaborn is used for its nice color palette\n",
    "year_counts = komaza_df['PlantYear'].value_counts().sort_index()\n",
    "# Generate a list of colors, one for each year\n",
    "colors = sns.color_palette('hsv', len(year_counts))\n",
    "\n",
    "# Plot with different colors for each year\n",
    "year_counts.plot(kind='bar', color=colors)\n",
    "plt.xlabel('Year of Plant')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Coastal Kenya sites Count of Entries for Each Year of Plant')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komaza_df[\"Coordinates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import simplekml\n",
    "import ast\n",
    "\n",
    "# Assuming komaza_df is already loaded\n",
    "output_folder = \"/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/kml_files\"\n",
    "\n",
    "def format_coordinates_to_list(coordinates):\n",
    "    # If coordinates are in string format, convert them to a list of tuples\n",
    "    if isinstance(coordinates, str):\n",
    "        coordinates = ast.literal_eval(coordinates)\n",
    "    # Convert each tuple to include longitude, latitude, and altitude (0)\n",
    "    return [(coord[1], coord[0], 0) for coord in coordinates]  # Note the order change to (lon, lat, alt)\n",
    "\n",
    "# Example usage with a DataFrame\n",
    "for index, row in komaza_df.iterrows():\n",
    "    kml = simplekml.Kml()\n",
    "    pol = kml.newpolygon(name=f\"Polygon_{index}\")\n",
    "    # Ensure the coordinates are in the correct format\n",
    "    formatted_coordinates = format_coordinates_to_list(row[\"Coordinates\"])\n",
    "    pol.outerboundaryis = formatted_coordinates  # Directly use the list of tuples\n",
    "    # Save the KML file in the specified output folder\n",
    "    kml.save(f\"{output_folder}/Polygon_{index}.kml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import simplekml\n",
    "import ast\n",
    "\n",
    "output_folder = \"/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/kml_files\"\n",
    "\n",
    "def format_coordinates_to_list(coordinates):\n",
    "    if isinstance(coordinates, str):\n",
    "        coordinates = ast.literal_eval(coordinates)\n",
    "    return [(coord[0], coord[1], 0) for coord in coordinates]\n",
    "\n",
    " \n",
    "kml_file_names = []\n",
    "\n",
    "for index, row in komaza_df.iterrows():\n",
    "    kml = simplekml.Kml()\n",
    "    pol = kml.newpolygon(name=f\"Polygon_{index}\")\n",
    "    formatted_coordinates = format_coordinates_to_list(row[\"Coordinates\"])\n",
    "    pol.outerboundaryis = formatted_coordinates\n",
    "    pol.style.polystyle.color = simplekml.Color.changealphaint(0, simplekml.Color.red)\n",
    "    pol.style.linestyle.width = 5\n",
    "    file_name = f\"Polygon_{index}.kml\"\n",
    "    kml.save(f\"{output_folder}/{file_name}\")\n",
    "    \n",
    "    kml_file_names.append(file_name)\n",
    " \n",
    "komaza_df['kml_file_name'] = kml_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komaza_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "from functools import partial\n",
    "import ast\n",
    "\n",
    "\n",
    "def calculate_area_sqkm(coordinates):\n",
    "\n",
    "    coordinates = ast.literal_eval(coordinates)\n",
    "    polygon = Polygon(coordinates)\n",
    "    project = partial(\n",
    "        pyproj.transform,\n",
    "        pyproj.Proj(init='epsg:4326'),  \n",
    "        pyproj.Proj(init='epsg:6933')  \n",
    "    )\n",
    "    projected_polygon = transform(project, polygon)\n",
    "    area_sqm = projected_polygon.area\n",
    "    area_sqkm = area_sqm / 1e6\n",
    "    return area_sqkm\n",
    "\n",
    "\n",
    "komaza_df['Area_sqkm'] = komaza_df['Coordinates'].apply(calculate_area_sqkm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komaza_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komaza_df.to_csv('/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/kml_files/Komaza_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "csv_file_path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/Komaza_preprocessed_sample - Komaza_preprocessed.csv'\n",
    "\n",
    "\n",
    "df_1 = pd.read_csv(csv_file_path)\n",
    "\n",
    "df_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/NEW_LANDCOVER_SAMPLE - Sheet1.csv\"\n",
    "df_2=pd.read_csv(path)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = pd.merge(df_1, df_2, left_on='kml_file_name', right_on='label')\n",
    "\n",
    "\n",
    "merged_df.to_csv('/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/Sample_Landcover.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/NEW_LANDCOVER_SAMPLE - Sheet1.csv'\n",
    "\n",
    "\n",
    "df_2 = pd.read_csv(csv_file_path)\n",
    "\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fastkml import kml\n",
    "from shapely.geometry import shape, Polygon\n",
    "import geopandas as gpd\n",
    "\n",
    "def kml_to_polygon(kml_path):\n",
    "    try:\n",
    "        with open(kml_path, 'rb') as file:  \n",
    "            doc = file.read()\n",
    "            k = kml.KML()\n",
    "            k.from_string(doc)  \n",
    "            features = list(k.features())\n",
    "            placemarks = list(features[0].features())\n",
    "            polygon = placemarks[0].geometry\n",
    "            if not isinstance(polygon, Polygon):\n",
    "                polygon = shape(polygon)\n",
    "            return polygon\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {kml_path}\")\n",
    "        return None\n",
    "\n",
    "geometries = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "kml_folder_path = \"/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/kml_files\"\n",
    "\n",
    "for index, row in komaza_df.iterrows():\n",
    "    kml_file_name = row['kml_file_name']\n",
    "    kml_path = os.path.join(kml_folder_path, kml_file_name)\n",
    "    polygon = kml_to_polygon(kml_path)\n",
    "    if polygon is not None:\n",
    "        geometries.append(polygon)\n",
    "        labels.append(row['kml_file_name'])\n",
    "    else:\n",
    "        print(f\"Skipping missing file: {kml_path}\")\n",
    "\n",
    "gdf = gpd.GeoDataFrame({'label': labels, 'geometry': geometries})\n",
    "gdf.crs = \"EPSG:4326\"\n",
    "gdf.to_file(\"/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/komaza_shapefile.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "\n",
    "def latlong_to_utm(coords):\n",
    "    proj_latlong = Proj(proj='latlong', datum='WGS84')\n",
    "    proj_utm = Proj(proj='utm', zone=33, datum='WGS84')   \n",
    "    utm_coords = [transform(proj_latlong, proj_utm, coord[1], coord[0]) for coord in coords]\n",
    "    return utm_coords\n",
    "\n",
    "def parse_polygon_coords(polygon_str):\n",
    "    coords = re.findall(r\"\\(([^)]+)\\)\", polygon_str)\n",
    "    return [tuple(map(float, coord.split(','))) for coord in coords]\n",
    "\n",
    "def calculate_polygon_area(coords):\n",
    "    utm_coords = latlong_to_utm(coords)\n",
    "    n = len(utm_coords)\n",
    "    area = 0.0\n",
    "    for i in range(n):\n",
    "        j = (i + 1) % n\n",
    "        area += utm_coords[i][0] * utm_coords[j][1]\n",
    "        area -= utm_coords[j][0] * utm_coords[i][1]\n",
    "    area = abs(area) / 2.0\n",
    "    return area / 10000   \n",
    "\n",
    "\n",
    "komaza_df['Area_in_ha'] = komaza_df['Coordinates'].apply(\n",
    "    lambda x: calculate_polygon_area(parse_polygon_coords(x)) if isinstance(x, str) else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fastkml import kml\n",
    "from shapely.geometry import shape, Polygon\n",
    "import geopandas as gpd\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "def kml_to_bbox(kml_path):\n",
    "    \"\"\"Extracts the bounding box from a KML file.\"\"\"\n",
    "    try:\n",
    "        with open(kml_path, 'rb') as file:\n",
    "            doc = file.read()\n",
    "            try:\n",
    "                \n",
    "                k = kml.KML()\n",
    "                try:\n",
    "                    k.from_string(doc)   \n",
    "                except etree.XMLSyntaxError as e:\n",
    "                    print(f\"XML parsing error in file {kml_path}: {e}\")\n",
    "                    return None\n",
    "                \n",
    "                features = list(k.features())\n",
    "                placemarks = list(features[0].features())\n",
    "                polygon = placemarks[0].geometry\n",
    "                if not isinstance(polygon, Polygon):\n",
    "                    polygon = shape(polygon)\n",
    "                return polygon.bounds   \n",
    "            except AttributeError as e:\n",
    "                print(f\"Error processing KML file {kml_path}: {e}\")\n",
    "                return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {kml_path}\")\n",
    "        return None\n",
    "\n",
    "def generate_planet_explorer_link(bbox):\n",
    "    \"\"\"Generates a link to Planet Explorer based on the bounding box.\"\"\"\n",
    "    minx, miny, maxx, maxy = bbox\n",
    "    center_lat = (miny + maxy) / 2\n",
    "    center_lon = (minx + maxx) / 2\n",
    "   \n",
    "    zoom_level = 12\n",
    "    return f\"https://www.planet.com/explorer/#/center/{center_lon},{center_lat}/zoom/{zoom_level}\"\n",
    "\n",
    "kml_folder_path = \"/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/kml_files\"\n",
    "links = []\n",
    "\n",
    "for kml_file in os.listdir(kml_folder_path):\n",
    "    kml_path = os.path.join(kml_folder_path, kml_file)\n",
    "    bbox = kml_to_bbox(kml_path)\n",
    "    if bbox:\n",
    "        link = generate_planet_explorer_link(bbox)\n",
    "        links.append(link)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating links from explorer \"https://support.planet.com/hc/en-us/articles/7506760029341-Sharing-Your-Session-in-Planet-Explorer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.parse\n",
    "from fastkml import kml\n",
    "from shapely.geometry import Polygon, shape, mapping\n",
    "\n",
    "def parse_kml_file(kml_path):\n",
    "    try:\n",
    "        with open(kml_path, 'rb') as file:\n",
    "            kml_content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {kml_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        k = kml.KML()\n",
    "        k.from_string(kml_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing KML from file {kml_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def find_polygon(features):\n",
    "        for feature in features:\n",
    "            if hasattr(feature, 'geometry') and feature.geometry.geom_type == 'Polygon':\n",
    "                return feature.geometry\n",
    "            elif hasattr(feature, 'features'):\n",
    "                polygon = find_polygon(feature.features())\n",
    "                if polygon:\n",
    "                    return polygon\n",
    "        return None\n",
    "\n",
    "    def extract_features(element):\n",
    "        if hasattr(element, 'features'):\n",
    "            return element.features()\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        features = []\n",
    "        for doc in k.features():\n",
    "            features.extend(extract_features(doc))\n",
    "        return find_polygon(features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding polygon in file {kml_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def construct_planet_explorer_url(polygon):\n",
    "    if not isinstance(polygon, Polygon):\n",
    "        polygon = shape(polygon)\n",
    "\n",
    "    try:\n",
    "        geojson_polygon = json.dumps(mapping(polygon))\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting polygon to GeoJSON: {e}\")\n",
    "        return None\n",
    "\n",
    "    encoded_geojson = urllib.parse.quote(geojson_polygon)\n",
    "    print(f\"Encoded GeoJSON: {encoded_geojson}\")\n",
    "\n",
    "    centroid = polygon.centroid\n",
    "    lat, lon = centroid.y, centroid.x\n",
    "\n",
    "    # Adjust zoom level if necessary based on the polygon size\n",
    "    zoom_level = 12\n",
    "\n",
    "    url = f\"https://www.planet.com/explorer/?geometry={encoded_geojson}&center={lat},{lon}&zoom={zoom_level}&show_boundaries=true\"\n",
    "    return url\n",
    "\n",
    "kml_directory = \"/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/kml_files\"\n",
    "\n",
    "kml_files = [f for f in os.listdir(kml_directory) if f.endswith('.kml')]\n",
    "\n",
    "for kml_file in kml_files:\n",
    "    kml_path = os.path.join(kml_directory, kml_file)\n",
    "    print(f\"Processing file: {kml_file}\")\n",
    "    polygon = parse_kml_file(kml_path)\n",
    "    \n",
    "    if polygon:\n",
    "        url = construct_planet_explorer_url(polygon)\n",
    "        if url:\n",
    "            print(f\"{kml_file}: {url}\")\n",
    "        else:\n",
    "            print(f\"{kml_file}: Error constructing URL\")\n",
    "    else:\n",
    "        print(f\"{kml_file}: No valid polygon found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Replace with your Planet API key\n",
    "API_KEY = 'PLAK37b7e85e0cec4b359fb4da4e9c62c3dd'\n",
    "\n",
    "def create_session(kml_path):\n",
    "    url = \"https://api.planet.com/data/v1/graph\"\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'api-key {API_KEY}'\n",
    "    }\n",
    "\n",
    "    with open(kml_path, 'r') as file:\n",
    "        kml_content = file.read()\n",
    "\n",
    "    payload = {\n",
    "        \"aoi\": kml_content,\n",
    "        \"type\": \"kml\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('links').get('self')\n",
    "    else:\n",
    "        print(f\"Error creating session for {kml_path}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "kml_directory = \"/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/kml_files\"\n",
    "\n",
    "kml_files = [f for f in os.listdir(kml_directory) if f.endswith('.kml')]\n",
    "\n",
    "for kml_file in kml_files:\n",
    "    kml_path = os.path.join(kml_directory, kml_file)\n",
    "    print(f\"Processing file: {kml_file}\")\n",
    "    session_link = create_session(kml_path)\n",
    "    \n",
    "    if session_link:\n",
    "        print(f\"{kml_file}: {session_link}\")\n",
    "    else:\n",
    "        print(f\"{kml_file}: Error creating session\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory containing your .kml files\n",
    "kml_directory = \"/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Coastal_Kenya_data/kml_files\"\n",
    "\n",
    "# Get a list of all .kml files in the directory\n",
    "kml_files = [f for f in os.listdir(kml_directory) if f.endswith('.kml')]\n",
    "\n",
    "# Iterate through each .kml file, parse it, and construct the URL\n",
    "for kml_file in kml_files:\n",
    "    kml_path = os.path.join(kml_directory, kml_file)\n",
    "    polygon = parse_kml_file(kml_path)\n",
    "    \n",
    "    if polygon:\n",
    "        url = construct_planet_explorer_url(polygon)\n",
    "        print(f\"{kml_file}: {url}\")\n",
    "    else:\n",
    "        print(f\"{kml_file}: No valid polygon found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area_ha = komaza_df['Area_in_ha'].sum()\n",
    "print(f\"Total area covered: {total_area_ha} hectares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latin America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/OTHER_KML.csv'\n",
    "\n",
    "\n",
    "latin= pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "latin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area_ha = latin['Hectarea'].sum()\n",
    "print(f\"Total area covered: {total_area_ha} hectares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "latin['Year'] = pd.to_datetime(latin['Fecha_plan']).dt.year\n",
    "\n",
    "year_counts = latin['Year'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "year_counts.plot(kind='bar', color=plt.cm.rainbow(np.linspace(0, 1, len(year_counts))))\n",
    "plt.title('Count per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Weyerhaeuser-Eucapine N960- Project Area.csv'\n",
    "\n",
    "\n",
    "WSE= pd.read_csv(file_path)\n",
    "\n",
    "WSE[\"coordinates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "def latlong_to_utm(coords):\n",
    "    proj_latlong = Proj(proj='latlong', datum='WGS84')\n",
    "    proj_utm = Proj(proj='utm', zone=33, datum='WGS84')  \n",
    "    utm_coords = [transform(proj_latlong, proj_utm, coord[1], coord[0]) for coord in coords]\n",
    "    return utm_coords\n",
    "\n",
    "def parse_polygon_coords(polygon_str):\n",
    "    coord_pairs = polygon_str.split()\n",
    "    coords = [tuple(map(float, coord.split(',')[:2])) for coord in coord_pairs]\n",
    "    return coords\n",
    "\n",
    "def calculate_polygon_area(coords):\n",
    "    utm_coords = latlong_to_utm(coords)\n",
    "    n = len(utm_coords)\n",
    "    area = 0.0\n",
    "    for i in range(n):\n",
    "        j = (i + 1) % n\n",
    "        area += utm_coords[i][0] * utm_coords[j][1]\n",
    "        area -= utm_coords[j][0] * utm_coords[i][1]\n",
    "    area = abs(area) / 2.0\n",
    "    return area / 10000  \n",
    "\n",
    "\n",
    "WSE['Area_in_ha'] = WSE['coordinates'].apply(\n",
    "    lambda x: calculate_polygon_area(parse_polygon_coords(x)) if isinstance(x, str) else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area_ha = WSE['Area_in_ha'].sum()\n",
    "print(f\"Total area covered: {total_area_ha} hectares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSE.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concatinating datasets with common columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/latin_projects'  \n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "\n",
    "for filename in all_files:\n",
    "\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "concatenated_df_latin = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df_latin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concatenated_df_latin['Area_in_ha'] = concatenated_df_latin['coordinates'].apply(\n",
    "    lambda x: calculate_polygon_area(parse_polygon_coords(x)) if isinstance(x, str) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df_latin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area_ha = concatenated_df_latin['Area_in_ha'].sum()\n",
    "print(f\"Total area covered: {total_area_ha} hectares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "filtered_projects= concatenated_df_latin.loc[concatenated_df_latin['Area_in_ha'] < 200]\n",
    "filtered_projects.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_projects.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area_ha = filtered_projects['Area_in_ha'].sum()\n",
    "print(f\"Total area covered: {total_area_ha} hectares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asia Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/Asia_Project'  \n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "\n",
    "for filename in all_files:\n",
    "\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "concatenated_df_Asia= pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df_Asia.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df_Asia.to_csv('/home/idisc02/Forest_Project_New/Forest_Monitoring/input/Verra_Projects/TIST_Projects_without_trees/concatenated_df_Asia.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area_ha = concatenated_df_Asia['Area'].sum()\n",
    "print(f\"Total area covered: {total_area_ha} hectares\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "downgrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
