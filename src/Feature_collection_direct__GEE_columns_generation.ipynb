{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYljfrMBwslv"
      },
      "source": [
        "**Introduction**\n",
        "Generating the Extra Columns wiith earth Engine API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "VFCs7UizgEfC",
        "outputId": "2b6771b4-2ad6-4d89-b5b3-e5b45c78c620"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import shape , mapping\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "from shapely import wkt\n",
        "import geemap\n",
        "import ee\n",
        "import json\n",
        "# from google.colab import auth , drive\n",
        "# drive.mount('/content/drive') Only required if you are using google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "lYlQ3kCMXlXV",
        "outputId": "6e070639-84b4-4dd6-cffc-6f77604517ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <style>\n",
              "                .geemap-dark {\n",
              "                    --jp-widgets-color: white;\n",
              "                    --jp-widgets-label-color: white;\n",
              "                    --jp-ui-font-color1: white;\n",
              "                    --jp-layout-color2: #454545;\n",
              "                    background-color: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-dark .jupyter-button {\n",
              "                    --jp-layout-color3: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-colab {\n",
              "                    background-color: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "\n",
              "                .geemap-colab .jupyter-button {\n",
              "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "            </style>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/geopandas/io/file.py:383: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
            "  as_dt = pd.to_datetime(df[k], errors=\"ignore\")\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/geopandas/io/file.py:387: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
            "  as_dt = pd.to_datetime(df[k], errors=\"ignore\", utc=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
            "RangeIndex: 801222 entries, 0 to 801221\n",
            "Data columns (total 16 columns):\n",
            " #   Column                    Non-Null Count   Dtype   \n",
            "---  ------                    --------------   -----   \n",
            " 0   project_id_reported       746726 non-null  object  \n",
            " 1   trees_planted_reported    13193 non-null   float64 \n",
            " 2   country                   795834 non-null  object  \n",
            " 3   description_reported      2548 non-null    object  \n",
            " 4   planting_date_reported    751711 non-null  object  \n",
            " 5   geometry_reported         801222 non-null  object  \n",
            " 6   survival_rate_reported    1269 non-null    float64 \n",
            " 7   site_sqkm                 801222 non-null  float64 \n",
            " 8   site_id_reported          757652 non-null  object  \n",
            " 9   host_name                 801222 non-null  object  \n",
            " 10  url                       801222 non-null  object  \n",
            " 11  species_count_reported    0 non-null       object  \n",
            " 12  species_planted_reported  270 non-null     object  \n",
            " 13  created_site_ids          801222 non-null  object  \n",
            " 14  created_project_ids       801222 non-null  object  \n",
            " 15  geometry                  801221 non-null  geometry\n",
            "dtypes: float64(3), geometry(1), object(12)\n",
            "memory usage: 97.8+ MB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df = gpd.read_file(\"../input/consolidated_reforestation_projects.geojson\")\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "YlOhnGuZbfeq",
        "outputId": "75405cd0-6730-4d81-95a2-04034e253199"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <style>\n",
              "                .geemap-dark {\n",
              "                    --jp-widgets-color: white;\n",
              "                    --jp-widgets-label-color: white;\n",
              "                    --jp-ui-font-color1: white;\n",
              "                    --jp-layout-color2: #454545;\n",
              "                    background-color: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-dark .jupyter-button {\n",
              "                    --jp-layout-color3: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-colab {\n",
              "                    background-color: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "\n",
              "                .geemap-colab .jupyter-button {\n",
              "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "            </style>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0         POLYGON ((-5561391.868 -1038670.909, -5561494....\n",
              "1         POLYGON Z ((-4839336.675 -2554053.909 0.000, -...\n",
              "2         POLYGON Z ((-4838167.931 -2552670.718 0.000, -...\n",
              "3         POLYGON Z ((-4838872.807 -2554348.003 0.000, -...\n",
              "4         POLYGON Z ((-224756.486 911808.834 0.000, -224...\n",
              "                                ...                        \n",
              "801217    POLYGON ((-6438930.160 -3609277.645, -6438930....\n",
              "801218    POLYGON ((-6105893.149 -3908169.191, -6105368....\n",
              "801219    POLYGON ((-6105696.076 -3907505.595, -6105716....\n",
              "801220    POLYGON ((-6129944.978 -3906375.852, -6129776....\n",
              "801221    POLYGON ((-6002151.683 -3813346.665, -6001723....\n",
              "Name: geometry, Length: 801222, dtype: geometry"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"geometry\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vXiObiAgxscR",
        "outputId": "afc4ca4e-e2be-403b-8fc8-4b2ff6cd57df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <style>\n",
              "                .geemap-dark {\n",
              "                    --jp-widgets-color: white;\n",
              "                    --jp-widgets-label-color: white;\n",
              "                    --jp-ui-font-color1: white;\n",
              "                    --jp-layout-color2: #454545;\n",
              "                    background-color: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-dark .jupyter-button {\n",
              "                    --jp-layout-color3: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-colab {\n",
              "                    background-color: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "\n",
              "                .geemap-colab .jupyter-button {\n",
              "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "            </style>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "ee.Initialize(project='spring-idiom-398208')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnbpOL6EbmK4"
      },
      "source": [
        "##TREE COVER COLUMNS; \"tree_cover_area_2000,tree_cover_area_2005 tree_cover_area_2010,tree_cover_area_2015 AND tree_cover_area_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "bwlZmsIgbaFh",
        "outputId": "3c26595c-6026-40ab-938a-f843bab0f63e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <style>\n",
              "                .geemap-dark {\n",
              "                    --jp-widgets-color: white;\n",
              "                    --jp-widgets-label-color: white;\n",
              "                    --jp-ui-font-color1: white;\n",
              "                    --jp-layout-color2: #454545;\n",
              "                    background-color: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-dark .jupyter-button {\n",
              "                    --jp-layout-color3: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-colab {\n",
              "                    background-color: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "\n",
              "                .geemap-colab .jupyter-button {\n",
              "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "            </style>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/geopandas/geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
            "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing chunk 1/802\n",
            "Could not convert the geojson to ee.Geometry()\n",
            "Error converting chunk 1 to Earth Engine FeatureCollection: Invalid GeoJSON geometry.\n",
            "Processing chunk 2/802\n",
            "Could not convert the geojson to ee.Geometry()\n",
            "Error converting chunk 2 to Earth Engine FeatureCollection: Invalid GeoJSON geometry.\n",
            "Processing chunk 3/802\n",
            "Exporting chunk 3...\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n",
            "Polling for task (id: KDCB2E2AL2B52SFF73OODRTR) of chunk 3.\n"
          ]
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "import json\n",
        "import ee\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import geemap\n",
        "from shapely.geometry import shape\n",
        "from shapely.validation import make_valid\n",
        "\n",
        "\n",
        "\n",
        "#  cleaning  and loading GeoJSON safely\n",
        "def safe_json_load(row):\n",
        "    if isinstance(row, str):\n",
        "        try:\n",
        "            geojson_dict = json.loads(row.replace(\"'\", '\"'))\n",
        "            geom = shape(geojson_dict)\n",
        "            return make_valid(geom).buffer(0)  \n",
        "        except json.JSONDecodeError:\n",
        "            return None\n",
        "    elif isinstance(row, dict):\n",
        "        geom = shape(row)\n",
        "        return make_valid(geom).buffer(0)  \n",
        "    else:\n",
        "        return row\n",
        "\n",
        "# Converting geometry column to valid 2D geometries\n",
        "df['geometry'] = df['geometry'].apply(safe_json_load)\n",
        "df = df.set_geometry('geometry')\n",
        "df = df.to_crs(epsg=4326)  # Ensuring CRS is WGS 84 (EPSG:4326)\n",
        "\n",
        "# Dropping any rows with invalid geometries\n",
        "df_valid = df.dropna(subset=['geometry'])\n",
        "\n",
        "gdf = gpd.GeoDataFrame(df_valid, geometry='geometry')\n",
        "\n",
        "# Spliting data into chunks for processing as earth engine has data loadinging limitataions\n",
        "chunk_size = 1000\n",
        "chunks = [gdf[i:i + chunk_size] for i in range(0, gdf.shape[0], chunk_size)]\n",
        "\n",
        "# Defining the  Earth Engine  GLAD landcover images\n",
        "landmask = ee.Image(\"projects/glad/OceanMask\").lte(1)\n",
        "landCover = ee.Image('projects/glad/GLCLU2020/v2/LCLUC_2020').updateMask(landmask)\n",
        "\n",
        "# Listing all class codes for GLAD land cover\n",
        "classCodes = [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,\n",
        "              125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]\n",
        "\n",
        "# Masking land cover image to only include class codes of interest for tree cover\n",
        "maskedLandCover = landCover.remap(classCodes, classCodes, 255)\n",
        "\n",
        "#  Calculating area for each class list in the codes\n",
        "def calculate_area(feature):\n",
        "    total_area = ee.Number(0)\n",
        "    def calculate_class_area(class_code, total):\n",
        "        class_area = maskedLandCover.eq(ee.Number(class_code)) \\\n",
        "            .multiply(ee.Image.pixelArea()) \\\n",
        "            .reduceRegion(\n",
        "                reducer=ee.Reducer.sum(),\n",
        "                geometry=feature.geometry(),\n",
        "                scale=30,\n",
        "                maxPixels=1e9\n",
        "            ).get('remapped')\n",
        "\n",
        "        area_value = ee.Number(class_area).divide(1e6)  # Converting the area to km^2\n",
        "        return ee.Number(total).add(area_value)\n",
        "\n",
        "    total_area = ee.List(classCodes).iterate(calculate_class_area, total_area)\n",
        "    return feature.set('tree_cover_area_2020', total_area)\n",
        "\n",
        "\n",
        "output_csv_path = '../input/Reforestation_Data/merged_land_cover_results.csv'\n",
        "\n",
        "#  combining the  results DataFrame\n",
        "if os.path.exists(output_csv_path):\n",
        "    combined_df = pd.read_csv(output_csv_path)\n",
        "else:\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "# Looping through chunks and process each in GEE\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "    # Converting GeoDataFrame chunk to GeoJSON and then Earth Engine FeatureCollection\n",
        "    gdf_json_chunk = chunk.__geo_interface__\n",
        "    \n",
        "    try:\n",
        "        # Converting to Earth Engine FeatureCollection\n",
        "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Mapping the area calculation function over the FeatureCollection\n",
        "    area_results_chunk = fc_chunk.map(calculate_area)\n",
        "\n",
        "    # Exporting each chunk to Google Drive as CSV\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=area_results_chunk,\n",
        "        description=f'asia_chunk_{i + 1}',\n",
        "        folder='Reforestation_Data',\n",
        "        fileNamePrefix=f'temp_chunk_{i + 1}_new_2020_land_cover',\n",
        "        fileFormat='CSV'\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "    print(f\"Exporting chunk {i + 1}...\")\n",
        "\n",
        "    while task.active():\n",
        "        print(f'Polling for task (id: {task.id}) of chunk {i + 1}.')\n",
        "        time.sleep(30)\n",
        "\n",
        "    print(f'Chunk {i + 1} export done.')\n",
        "\n",
        "    # Loading the exported CSV for this chunk\n",
        "    temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{i + 1}_new_2020_land_cover.csv'\n",
        "    temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "\n",
        "    # Appending to combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "    # Saving combined results to the output CSV\n",
        "    combined_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    # Removing the temporary file\n",
        "    if os.path.exists(temp_chunk_path):\n",
        "        os.remove(temp_chunk_path)\n",
        "        print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "print(\"All chunks processed, combined results saved to:\", output_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9bzuuIzb7E-"
      },
      "source": [
        "##CHANGE COLUMNS EXTRACTION (\"permanent_water\", \"short_vegetation_after_tree_loss\", \"cropland_loss_to_tree\", \"cropland_gain_from_trees\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Defining the Earth Engine GLAD landcover images\n",
        "landmask = ee.Image(\"projects/glad/OceanMask\").lte(1)\n",
        "landCover = ee.Image('projects/glad/GLCLU2020/v2/LCLUC').updateMask(landmask)\n",
        "\n",
        "# Class codes and names of interest\n",
        "classCodes = [208, 240, 248, 245]\n",
        "classesOfInterest = [\"permanent_water\", \"short_vegetation_after_tree_loss\", \"cropland_loss_to_tree\", \"cropland_gain_from_trees\"]\n",
        "\n",
        "# Masking land cover image to only include class codes of interest\n",
        "maskedLandCover = landCover.remap(classCodes, classCodes, 255)\n",
        "\n",
        "# Calculating area for each class\n",
        "def calculate_area(feature):\n",
        "    total_area = ee.Number(0)\n",
        "    def calculate_class_area(class_code, total):\n",
        "        class_area = maskedLandCover.eq(ee.Number(class_code)) \\\n",
        "            .multiply(ee.Image.pixelArea()) \\\n",
        "            .reduceRegion(\n",
        "                reducer=ee.Reducer.sum(),\n",
        "                geometry=feature.geometry(),\n",
        "                scale=30,\n",
        "                maxPixels=1e9\n",
        "            ).get('remapped')\n",
        "\n",
        "        area_value = ee.Number(class_area).divide(1e6)  # Converting area to km^2\n",
        "        return ee.Number(total).add(area_value)\n",
        "\n",
        "    total_area = ee.List(classCodes).iterate(calculate_class_area, total_area)\n",
        "    return feature.set('land_cover_area_2020', total_area)\n",
        "\n",
        "\n",
        "output_csv_path = '../input/Reforestation_Data/extra_land_cover_results.csv'\n",
        "\n",
        "# Combining the results DataFrame\n",
        "if os.path.exists(output_csv_path):\n",
        "    combined_df = pd.read_csv(output_csv_path)\n",
        "else:\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "# Looping through chunks and processing each in GEE\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "    # Converting GeoDataFrame chunk to GeoJSON and then Earth Engine FeatureCollection\n",
        "    gdf_json_chunk = chunk.__geo_interface__\n",
        "    \n",
        "    try:\n",
        "        # Converting to Earth Engine FeatureCollection\n",
        "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Mapping the area calculation function over the FeatureCollection\n",
        "    area_results_chunk = fc_chunk.map(calculate_area)\n",
        "\n",
        "    # Exporting each chunk to Google Drive as CSV\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=area_results_chunk,\n",
        "        description=f'asia_chunk_{i + 1}',\n",
        "        folder='Reforestation_Data',\n",
        "        fileNamePrefix=f'temp_chunk_{i + 1}_lulc_2020_land_cover',\n",
        "        fileFormat='CSV'\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "    print(f\"Exporting chunk {i + 1}...\")\n",
        "\n",
        "    while task.active():\n",
        "        print(f'Polling for task (id: {task.id}) of chunk {i + 1}.')\n",
        "        time.sleep(30)\n",
        "\n",
        "    print(f'Chunk {i + 1} export done.')\n",
        "\n",
        "    # Loading the exported CSV for this chunk\n",
        "    temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{i + 1}_lulc_2020_land_cover.csv'\n",
        "    temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "\n",
        "    # Appending to combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "    # Saving combined results to the output CSV\n",
        "    combined_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    # Removing the temporary file\n",
        "    if os.path.exists(temp_chunk_path):\n",
        "        os.remove(temp_chunk_path)\n",
        "        print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "print(\"All chunks processed, combined results saved to:\", output_csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6SiSH95KT9z"
      },
      "outputs": [],
      "source": [
        "\n",
        "# landmask = ee.Image(\"projects/glad/OceanMask\").lte(1)\n",
        "# landCover = ee.Image('projects/glad/GLCLU2020/v2/LCLUC').updateMask(landmask)\n",
        "\n",
        "# # Class codes and names of interest\n",
        "# classCodes = [208, 240, 248, 245]\n",
        "# classesOfInterest = [\"permanent_water\", \"short_vegetation_after_tree_loss\", \"cropland_loss_to_tree\", \"cropland_gain_from_trees\"]\n",
        "\n",
        "# maskedLandCover = landCover.remap(classCodes, classCodes, 255)\n",
        "\n",
        "# def calculate_area(feature):\n",
        "#     areas = {}\n",
        "\n",
        "\n",
        "#     for class_code, class_name in zip(classCodes, classesOfInterest):\n",
        "#         current_class_area = maskedLandCover.eq(class_code) \\\n",
        "#             .multiply(ee.Image.pixelArea()) \\\n",
        "#             .reduceRegion(\n",
        "#                 reducer=ee.Reducer.sum(),\n",
        "#                 geometry=feature.geometry(),\n",
        "#                 scale=30,\n",
        "#                 maxPixels=1e9\n",
        "#             )\n",
        "\n",
        "#         area_value = ee.Number(current_class_area.get('remapped', 0)).divide(1e6)  # Convert to km^2\n",
        "#         areas[class_name] = area_value\n",
        "\n",
        "#     return feature.set(areas)\n",
        "\n",
        "# output_csv_path = '../input/Reforestation_Data/extra_land_cover_results.csv'\n",
        "\n",
        "\n",
        "# if os.path.exists(output_csv_path):\n",
        "#     combined_df = pd.read_csv(output_csv_path)\n",
        "# else:\n",
        "#     combined_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "# for i, fc_chunk in enumerate(chunks):\n",
        "#     print(f\"Processing chunk {i + 1}...\")\n",
        "\n",
        "\n",
        "#     area_results_chunk = fc_chunk.map(calculate_area)\n",
        "\n",
        "\n",
        "#     task = ee.batch.Export.table.toDrive(\n",
        "#         collection=area_results_chunk,\n",
        "#         description=f'extra_land_cover_chunk_{i + 1}',\n",
        "#         folder='Reforestation_Data',\n",
        "#         fileNamePrefix=f'extra_land_cover_chunk_{i + 1}_with_areas',\n",
        "#         fileFormat='CSV'\n",
        "#     )\n",
        "#     task.start()\n",
        "\n",
        "#     print(f\"Exporting processed chunk {i + 1}...\")\n",
        "\n",
        "#     while task.active():\n",
        "#         print(f'Polling for task (id: {task.id}) of processed chunk {i + 1}.')\n",
        "#         time.sleep(30)\n",
        "\n",
        "#     print(f'Processed chunk {i + 1} export done.')\n",
        "\n",
        "\n",
        "#     processed_chunk_path = f'../input/Reforestation_Data/extra_land_cover_chunk_{i + 1}_with_areas.csv'\n",
        "#     processed_chunk_df = pd.read_csv(processed_chunk_path)\n",
        "\n",
        "\n",
        "#     combined_df = pd.concat([combined_df, processed_chunk_df], ignore_index=True)\n",
        "\n",
        "#     combined_df.to_csv(output_csv_path, index=False)\n",
        "#             # Removing the temporary chunk file after processing\n",
        "#     if os.path.exists(temp_chunk_path):\n",
        "#         os.remove(temp_chunk_path)\n",
        "#         print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "# print(\"All processed chunks saved to:\", output_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGtZpPxwuY5x"
      },
      "source": [
        "##BUILT AREA CHARACTERISTICS EXTRACTION(BUILT_AREA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Output CSV path for built area results\n",
        "output_csv_path = '../input/Reforestation_Data/merged_built_area_results.csv'\n",
        "\n",
        "# Check if there's already an existing combined CSV\n",
        "if os.path.exists(output_csv_path):\n",
        "    combined_df = pd.read_csv(output_csv_path)\n",
        "else:\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "# Loading the Earth Engine built area image for 2018\n",
        "builtImage = ee.Image(\"JRC/GHSL/P2023A/GHS_BUILT_C/2018\").select('built_characteristics')\n",
        "\n",
        "# calculating the built area for each feature\n",
        "def calculate_built_area(feature):\n",
        "    polygon = feature.geometry()\n",
        "    built_area = builtImage.reduceRegion(\n",
        "        reducer=ee.Reducer.sum(),\n",
        "        geometry=polygon,\n",
        "        scale=10,\n",
        "        maxPixels=1e9\n",
        "    ).get('built_characteristics')\n",
        "\n",
        "    built_area = ee.Number(built_area).max(0)\n",
        "    return feature.set('built_area', built_area)\n",
        "\n",
        "# Looping through chunks to process each chunk's built area\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing built area for chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "    # Converting GeoDataFrame chunk to GeoJSON and then Earth Engine FeatureCollection\n",
        "    gdf_json_chunk = chunk.__geo_interface__\n",
        "\n",
        "    try:\n",
        "        # Converting the GeoJSON chunk to Earth Engine FeatureCollection\n",
        "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Mapping the built area calculation function over the FeatureCollection\n",
        "    built_area_results_chunk = fc_chunk.map(calculate_built_area)\n",
        "\n",
        "    # Exporting each chunk to Google Drive as CSV\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=built_area_results_chunk,\n",
        "        description=f'asia_chunk_built_area_{i + 1}',\n",
        "        folder='Reforestation_Data',\n",
        "        fileNamePrefix=f'temp_chunk_{i + 1}_built_area_2018',\n",
        "        fileFormat='CSV'\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "    print(f\"Exporting built area for chunk {i + 1}...\")\n",
        "\n",
        "    # Polling the task until it's finished\n",
        "    while task.active():\n",
        "        print(f'Polling for task (id: {task.id}) of chunk {i + 1}.')\n",
        "        time.sleep(30)\n",
        "\n",
        "    print(f'Chunk {i + 1} export done.')\n",
        "\n",
        "    # Loading the exported CSV for this chunk\n",
        "    temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{i + 1}_built_area_2018.csv'\n",
        "    temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "\n",
        "    # Appending the current chunk results to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "    # Saving combined results to the output CSV\n",
        "    combined_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    # Removing the temporary chunk file after processing\n",
        "    if os.path.exists(temp_chunk_path):\n",
        "        os.remove(temp_chunk_path)\n",
        "        print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "print(\"All chunks processed, built area results saved to:\", output_csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9CpPXyoPKRU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# output_csv_path = '../input/Reforestation_Data/merged_built_area_results.csv'\n",
        "\n",
        "\n",
        "# if os.path.exists(output_csv_path):\n",
        "#     combined_df = pd.read_csv(output_csv_path)\n",
        "# else:\n",
        "#     combined_df = pd.DataFrame()\n",
        "\n",
        "# builtImage = ee.Image(\"JRC/GHSL/P2023A/GHS_BUILT_C/2018\").select('built_characteristics')\n",
        "\n",
        "# def calculate_built_area(feature):\n",
        "#     polygon = feature.geometry()\n",
        "#     built_area = builtImage.reduceRegion(\n",
        "#         reducer=ee.Reducer.sum(),\n",
        "#         geometry=polygon,\n",
        "#         scale=10,\n",
        "#         maxPixels=1e9\n",
        "#     ).get('built_characteristics')\n",
        "\n",
        "#     built_area = ee.Number(built_area).max(0)\n",
        "#     return feature.set('built_area', built_area)\n",
        "\n",
        "\n",
        "# for i, chunk in enumerate(chunks):\n",
        "#     print(f\"Processing built area for chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "#     gdf_json_chunk = chunk.__geo_interface__\n",
        "\n",
        "#     fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "#     built_area_results_chunk = fc_chunk.map(calculate_built_area)\n",
        "#     task = ee.batch.Export.table.toDrive(\n",
        "#         collection=built_area_results_chunk,\n",
        "#         description=f'asia_chunk_built_area_{i + 1}',\n",
        "#         folder='Reforestation_Data',\n",
        "#         fileNamePrefix=f'temp_chunk_{i + 1}_built_area_2018',\n",
        "#         fileFormat='CSV'\n",
        "#     )\n",
        "#     task.start()\n",
        "\n",
        "#     print(f\"Exporting built area for chunk {i + 1}...\")\n",
        "\n",
        "#     while task.active():\n",
        "#         print(f'Polling for task (id: {task.id}) of chunk {i + 1}.')\n",
        "#         time.sleep(30)\n",
        "\n",
        "#     print(f'Chunk {i + 1} export done.')\n",
        "\n",
        "#     temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{i + 1}_built_area_2018.csv'\n",
        "#     temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "\n",
        "#     combined_df = pd.concat([combined_df, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "#     combined_df.to_csv(output_csv_path, index=False)\n",
        "#             # Removing the temporary chunk file after processing\n",
        "#     if os.path.exists(temp_chunk_path):\n",
        "#         os.remove(temp_chunk_path)\n",
        "#         print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "# print(\"All chunks processed, built area results saved to:\", output_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-oBg_Hhuhcy"
      },
      "source": [
        "##ROAD NETWORKS COLUMNS(total_road_length_km)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "output_csv_path = '../input/Reforestation_Data/merged_road_length_results.csv'\n",
        "\n",
        "# Checing if there's already an existing combined CSV\n",
        "if os.path.exists(output_csv_path):\n",
        "    combined_df = pd.read_csv(output_csv_path)\n",
        "else:\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "# Loading the Earth Engine road datasets\n",
        "roadsAfrica = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-africa')\n",
        "roadsAmericas = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-americas')\n",
        "roadsAsia = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-asia')\n",
        "roadsEurope = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-europe')\n",
        "roadsOceaniaEast = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-oceania-east')\n",
        "roadsOceaniaWest = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-oceania-west')\n",
        "\n",
        "# Merging all road datasets into a single collection\n",
        "roads = roadsAfrica.merge(roadsAmericas).merge(roadsAsia).merge(roadsEurope).merge(roadsOceaniaEast).merge(roadsOceaniaWest)\n",
        "\n",
        "#  Calculating road length for each feature (polygon)\n",
        "def calculate_road_length(polygon):\n",
        "    # Finding all roads that intersect the polygon\n",
        "    intersecting_roads = roads.filterBounds(polygon.geometry())\n",
        "\n",
        "    #  clipping road geometry and calculating its length within the polygon\n",
        "    def clip_and_calculate_length(road):\n",
        "        road_geom = road.geometry()\n",
        "        polygon_geom = polygon.geometry()\n",
        "        clipped = road_geom.intersection(polygon_geom, ee.ErrorMargin(1))\n",
        "        return ee.Feature(clipped).set('length', clipped.length())\n",
        "\n",
        "    # Mapping the clipping and length calculation over the intersecting roads\n",
        "    clipped_roads = intersecting_roads.map(clip_and_calculate_length)\n",
        "\n",
        "    # Summing the total road length in the polygon\n",
        "    road_length_sum = clipped_roads.reduceColumns(\n",
        "        reducer=ee.Reducer.sum(),\n",
        "        selectors=['length']\n",
        "    ).get('sum')\n",
        "\n",
        "    # Counting the number of intersecting roads\n",
        "    intersecting_roads_count = intersecting_roads.size()\n",
        "\n",
        "    # ting total road length (in km) and road count as properties on the polygon\n",
        "    return polygon.set({\n",
        "        'total_road_length_km': ee.Number(road_length_sum).divide(1000),\n",
        "        'intersecting_roads_count': intersecting_roads_count\n",
        "    })\n",
        "\n",
        "# Looping through chunks to process road lengths for each chunk\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing road length for chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "    # Converting the current GeoDataFrame chunk to GeoJSON\n",
        "    gdf_json_chunk = chunk.__geo_interface__\n",
        "\n",
        "    try:\n",
        "        # Converting the GeoJSON chunk to Earth Engine FeatureCollection\n",
        "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Mapping the road length calculation function over the FeatureCollection\n",
        "    road_length_results_chunk = fc_chunk.map(calculate_road_length)\n",
        "\n",
        "    # Exporting each chunk to Google Drive as CSV\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=road_length_results_chunk,\n",
        "        description=f'chunk_road_length_{i + 1}',\n",
        "        folder='Reforestation_Data',\n",
        "        fileNamePrefix=f'temp_chunk_{i + 1}_road_length',\n",
        "        fileFormat='CSV'\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "    print(f\"Exporting road length for chunk {i + 1}...\")\n",
        "\n",
        "    # Polling the task until it's finished\n",
        "    while task.active():\n",
        "        print(f'Polling for task (id: {task.id}) of chunk {i + 1}.')\n",
        "        time.sleep(30)\n",
        "\n",
        "    print(f'Chunk {i + 1} export done.')\n",
        "\n",
        "    # Loading the exported CSV for this chunk\n",
        "    temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{i + 1}_road_length.csv'\n",
        "    temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "\n",
        "    # Appending the current chunk results to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "    # Saving combined results to the output CSV\n",
        "    combined_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    # Removing the temporary chunk file after processing\n",
        "    if os.path.exists(temp_chunk_path):\n",
        "        os.remove(temp_chunk_path)\n",
        "        print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "print(\"All chunks processed, road length results saved to:\", output_csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdqZTYWrPrTq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# output_csv_path = '../input/Reforestation_Data/merged_road_length_results.csv'\n",
        "\n",
        "# if os.path.exists(output_csv_path):\n",
        "#     combined_df = pd.read_csv(output_csv_path)\n",
        "# else:\n",
        "#     combined_df = pd.DataFrame()\n",
        "\n",
        "# roadsAfrica = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-africa')\n",
        "# roadsAmericas = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-americas')\n",
        "# roadsAsia = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-asia')\n",
        "# roadsEurope = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-europe')\n",
        "# roadsOceaniaEast = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-oceania-east')\n",
        "# roadsOceaniaWest = ee.FeatureCollection('projects/ee-forest-monitoring/assets/gROADS-v1-oceania-west')\n",
        "\n",
        "# roads = roadsAfrica.merge(roadsAmericas).merge(roadsAsia).merge(roadsEurope).merge(roadsOceaniaEast).merge(roadsOceaniaWest)\n",
        "\n",
        "\n",
        "# def calculate_road_length(polygon):\n",
        "\n",
        "#     intersecting_roads = roads.filterBounds(polygon.geometry())\n",
        "\n",
        "#     def clip_and_calculate_length(road):\n",
        "#         road_geom = road.geometry()\n",
        "#         polygon_geom = polygon.geometry()\n",
        "#         clipped = road_geom.intersection(polygon_geom, ee.ErrorMargin(1))\n",
        "#         return ee.Feature(clipped).set('length', clipped.length())\n",
        "\n",
        "#     clipped_roads = intersecting_roads.map(clip_and_calculate_length)\n",
        "\n",
        "\n",
        "#     road_length_sum = clipped_roads.reduceColumns(\n",
        "#         reducer=ee.Reducer.sum(),\n",
        "#         selectors=['length']\n",
        "#     ).get('sum')\n",
        "\n",
        "\n",
        "#     intersecting_roads_count = intersecting_roads.size()\n",
        "\n",
        "\n",
        "#     return polygon.set({\n",
        "#         'total_road_length_km': ee.Number(road_length_sum).divide(1000),\n",
        "#         'intersecting_roads_count': intersecting_roads_count\n",
        "#     })\n",
        "\n",
        "\n",
        "# for i, chunk in enumerate(chunks):\n",
        "#     print(f\"Processing road length for chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "\n",
        "#     gdf_json_chunk = chunk.__geo_interface__\n",
        "\n",
        "\n",
        "#     fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "\n",
        "\n",
        "#     road_length_results_chunk = fc_chunk.map(calculate_road_length)\n",
        "\n",
        "\n",
        "#     task = ee.batch.Export.table.toDrive(\n",
        "#         collection=road_length_results_chunk,\n",
        "#         description=f'asia_chunk_road_length_{i + 1}',\n",
        "#         folder='Reforestation_Data',\n",
        "#         fileNamePrefix=f'temp_chunk_{i + 1}_road_length',\n",
        "#         fileFormat='CSV'\n",
        "#     )\n",
        "#     task.start()\n",
        "\n",
        "#     print(f\"Exporting road length for chunk {i + 1}...\")\n",
        "\n",
        "\n",
        "#     while task.active():\n",
        "#         print(f'Polling for task (id: {task.id}) of chunk {i + 1}.')\n",
        "#         time.sleep(30)\n",
        "\n",
        "#     print(f'Chunk {i + 1} export done.')\n",
        "\n",
        "\n",
        "#     temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{i + 1}_road_length.csv'\n",
        "#     temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "\n",
        "\n",
        "#     combined_df = pd.concat([combined_df, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "\n",
        "#     combined_df.to_csv(output_csv_path, index=False)\n",
        "#             # Removing the temporary chunk file after processing\n",
        "#     if os.path.exists(temp_chunk_path):\n",
        "#         os.remove(temp_chunk_path)\n",
        "#         print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "# print(\"All chunks processed, road length results saved to:\", output_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCScIwE2vgn_"
      },
      "source": [
        "##GLOBAL FOREST LOSS GROUPS EXTRACTION,using the python code in the \"data_analysis.ipny\" create the colun s \"loss_pre_5\",\"loss_post_3\",loss_post_5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "output_csv_path = '../input/Reforestation_Data/merged_forest_loss_results.csv'\n",
        "\n",
        "# Checking if there's already an existing combined CSV\n",
        "if os.path.exists(output_csv_path):\n",
        "    combined_df = pd.read_csv(output_csv_path)\n",
        "else:\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "# Loading the Global Forest Change 2023 dataset\n",
        "gfc2017 = ee.Image('UMD/hansen/global_forest_change_2023_v1_11')\n",
        "\n",
        "# Calculate forest loss within each feature (polygon)\n",
        "def calculate_loss(feature):\n",
        "    # Selecting the forest loss band and calculate loss area\n",
        "    loss_image = gfc2017.select(['loss'])\n",
        "    loss_area_image = loss_image.multiply(ee.Image.pixelArea())\n",
        "    loss_year = gfc2017.select(['lossyear'])\n",
        "\n",
        "    # Calculating forest loss area by year within the feature geometry\n",
        "    loss_by_year = loss_area_image.addBands(loss_year).reduceRegion(\n",
        "        reducer=ee.Reducer.sum().group(groupField=1),\n",
        "        geometry=feature.geometry(),\n",
        "        scale=30,\n",
        "        maxPixels=1e9\n",
        "    )\n",
        "\n",
        "   \n",
        "    return feature.set(loss_by_year)\n",
        "\n",
        "# Looping through chunks to process forest loss for each chunk\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing forest loss for chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "    # Converting the current GeoDataFrame chunk to GeoJSON\n",
        "    gdf_json_chunk = chunk.__geo_interface__\n",
        "\n",
        "    try:\n",
        "        # Converting GeoJSON chunk to Earth Engine FeatureCollection\n",
        "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting chunk {i + 1} to Earth Engine FeatureCollection: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Mapping the forest loss calculation function over the FeatureCollection\n",
        "    loss_results_chunk = fc_chunk.map(calculate_loss)\n",
        "\n",
        "    # Exporting each chunk's results to Google Drive as CSV\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=loss_results_chunk,\n",
        "        description=f'forest_loss_chunk_{i + 1}',\n",
        "        folder='Reforestation_Data',\n",
        "        fileNamePrefix=f'temp_chunk_{i + 1}_forest_loss',\n",
        "        fileFormat='CSV'\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "    print(f\"Exporting forest loss for chunk {i + 1}...\")\n",
        "\n",
        "    # Polling the task until it is completed\n",
        "    while task.active():\n",
        "        print(f'Polling for task (id: {task.id}) of chunk {i + 1}.')\n",
        "        time.sleep(30)\n",
        "\n",
        "    print(f'Chunk {i + 1} export done.')\n",
        "\n",
        "    # Loading the exported CSV for this chunk\n",
        "    temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{i + 1}_forest_loss.csv'\n",
        "    temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "\n",
        "    # Appending the current chunk's results to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "    # Saving the combined results to the output CSV\n",
        "    combined_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    # Removing the temporary chunk file after processing\n",
        "    if os.path.exists(temp_chunk_path):\n",
        "        os.remove(temp_chunk_path)\n",
        "        print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "print(\"All chunks processed, forest loss results saved to:\", output_csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWkbsB8YQHxJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# gfc2017 = ee.Image('UMD/hansen/global_forest_change_2023_v1_11')\n",
        "\n",
        "\n",
        "# output_csv_path = '../input/Reforestation_Data/merged_forest_loss_results.csv'\n",
        "\n",
        "\n",
        "# if os.path.exists(output_csv_path):\n",
        "#     combined_df = pd.read_csv(output_csv_path)\n",
        "# else:\n",
        "#     combined_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "# def calculate_loss(feature):\n",
        "#     loss_image = gfc2017.select(['loss'])\n",
        "#     loss_area_image = loss_image.multiply(ee.Image.pixelArea())\n",
        "#     loss_year = gfc2017.select(['lossyear'])\n",
        "\n",
        "#     loss_by_year = loss_area_image.addBands(loss_year).reduceRegion(\n",
        "#         reducer=ee.Reducer.sum().group(groupField=1),\n",
        "#         geometry=feature.geometry(),\n",
        "#         scale=30,\n",
        "#         maxPixels=1e9\n",
        "#     )\n",
        "\n",
        "#     return feature.set(loss_by_year)\n",
        "\n",
        "# for i, chunk in enumerate(chunks):\n",
        "#     print(f\"Processing forest loss for chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "\n",
        "#     gdf_json_chunk = chunk.__geo_interface__\n",
        "\n",
        "\n",
        "#     fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "\n",
        "\n",
        "#     loss_results_chunk = fc_chunk.map(calculate_loss)\n",
        "\n",
        "\n",
        "#     task = ee.batch.Export.table.toDrive(\n",
        "#         collection=loss_results_chunk,\n",
        "#         description=f'forest_loss_chunk_{i + 1}',\n",
        "#         folder='Reforestation_Data',\n",
        "#         fileNamePrefix=f'temp_chunk_{i + 1}_forest_loss',\n",
        "#         fileFormat='CSV'\n",
        "#     )\n",
        "#     task.start()\n",
        "\n",
        "#     print(f\"Exporting forest loss for chunk {i + 1}...\")\n",
        "\n",
        "\n",
        "#     while task.active():\n",
        "#         print(f'Polling for task (id: {task.id}) of chunk {i + 1}.')\n",
        "#         time.sleep(30)\n",
        "\n",
        "#     print(f'Chunk {i + 1} export done.')\n",
        "\n",
        "#     temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{i + 1}_forest_loss.csv'\n",
        "#     temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "\n",
        "\n",
        "#     combined_df = pd.concat([combined_df, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "\n",
        "#     combined_df.to_csv(output_csv_path, index=False)\n",
        "#             # Removing the temporary chunk file after processing\n",
        "#     if os.path.exists(temp_chunk_path):\n",
        "#         os.remove(temp_chunk_path)\n",
        "#         print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "# print(\"All chunks processed, forest loss results saved to:\", output_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWy6FPvg_Tdp"
      },
      "source": [
        "#### Generating the loss columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "8DWOivgh_S2q",
        "outputId": "2b6ada7b-bffc-4f50-a66e-de1239cc512a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>groups</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>[{group=15, sum=29764.3157698089}, {group=16, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1072</th>\n",
              "      <td>[{group=7, sum=5644.555928548177}, {group=10, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1073</th>\n",
              "      <td>[{group=3, sum=5151.752848307291}, {group=6, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1074</th>\n",
              "      <td>[{group=15, sum=793.3899449965534}, {group=16,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>[{group=1, sum=1816.9745761048562}, {group=2, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1076 rows  1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0                                                      []\n",
              "1                                                      []\n",
              "2                                                      []\n",
              "3                                                      []\n",
              "4                                                      []\n",
              "                              ...                        \n",
              "1071    [{group=15, sum=29764.3157698089}, {group=16, ...\n",
              "1072    [{group=7, sum=5644.555928548177}, {group=10, ...\n",
              "1073    [{group=3, sum=5151.752848307291}, {group=6, s...\n",
              "1074    [{group=15, sum=793.3899449965534}, {group=16,...\n",
              "1075    [{group=1, sum=1816.9745761048562}, {group=2, ...\n",
              "Name: groups, Length: 1076, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df_km=pd.read_csv('../input/Reforestation_Data/merged_forest_loss_results.csv')\n",
        "\n",
        "df_km[\"groups\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFDdvD00rk9B",
        "outputId": "cc1922de-b385-4c23-f97f-6dbf96d885c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1076 entries, 0 to 1075\n",
            "Data columns (total 11 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   system:index  1076 non-null   object \n",
            " 1   country       1076 non-null   object \n",
            " 2   descriptio    1076 non-null   object \n",
            " 3   groups        1076 non-null   object \n",
            " 4   planting_d    873 non-null    object \n",
            " 5   project_id    1076 non-null   object \n",
            " 6   site_id_re    1076 non-null   object \n",
            " 7   site_sqkm     1076 non-null   float64\n",
            " 8   survival_r    887 non-null    float64\n",
            " 9   trees_plan    1076 non-null   float64\n",
            " 10  .geo          1076 non-null   object \n",
            "dtypes: float64(3), object(8)\n",
            "memory usage: 92.6+ KB\n"
          ]
        }
      ],
      "source": [
        "df_km.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYt8cGNr_Spa"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "def parse_loss_groups(loss_groups):\n",
        "\n",
        "    all_groups = {f\"group_{i}\": 0 for i in range(24)}  # defining the 24 groups (0 to 23)-2000 to 2023\n",
        "\n",
        "    if not loss_groups or loss_groups == '[]':\n",
        "        return all_groups\n",
        "\n",
        "    loss_groups = loss_groups.replace('group=', '\"group\":').replace('sum=', '\"sum\":')\n",
        "    parsed_groups = ast.literal_eval(loss_groups)\n",
        "\n",
        "\n",
        "    for item in parsed_groups:\n",
        "        all_groups[f\"group_{item['group']}\"] = item['sum']\n",
        "\n",
        "    return all_groups\n",
        "\n",
        "\n",
        "expanded_columns = df_km['groups'].apply(parse_loss_groups)\n",
        "\n",
        "\n",
        "expanded_df = pd.DataFrame(expanded_columns.tolist())\n",
        "\n",
        "\n",
        "group_mapping = {\n",
        "    0: '2000',\n",
        "    1: '2001',\n",
        "    2: '2002',\n",
        "    3: '2003',\n",
        "    4: '2004',\n",
        "    5: '2005',\n",
        "    6: '2006',\n",
        "    7: '2007',\n",
        "    8: '2008',\n",
        "    9: '2009',\n",
        "    10: '2010',\n",
        "    11: '2011',\n",
        "    12: '2012',\n",
        "    13: '2013',\n",
        "    14: '2014',\n",
        "    15: '2015',\n",
        "    16: '2016',\n",
        "    17: '2017',\n",
        "    18: '2018',\n",
        "    19: '2019',\n",
        "    20: '2020',\n",
        "    21: '2021',\n",
        "    22: '2022',\n",
        "    23: '2023'\n",
        "}\n",
        "\n",
        "\n",
        "expanded_df.rename(columns=lambda x: group_mapping.get(int(x.split('_')[1]), x), inplace=True)\n",
        "\n",
        "\n",
        "df_km = df_km.join(expanded_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTCzExCYwGOw"
      },
      "outputs": [],
      "source": [
        "#  When only year is given in planting_date\n",
        "df_km[\"planting_date_reported\"]=df_km[\"planting_d\"]\n",
        "# def calculate_loss_pre_5(row):\n",
        "#     planted_year = row['planting_date_reported']\n",
        "#     years = [str(planted_year - i) for i in range(1, 6)]\n",
        "#     losses = [row[year] for year in years if year in df_km.columns]\n",
        "#     return np.nanmean(losses) if losses else np.nan\n",
        "\n",
        "# def calculate_loss_post_3(row):\n",
        "#     planted_year = row['planting_date_reported']\n",
        "#     years = [str(planted_year + i) for i in range(1, 4)]\n",
        "#     losses = [row[year] for year in years if year in df_km.columns]\n",
        "#     return np.nanmean(losses) if losses else np.nan\n",
        "# def calculate_loss_post_5(row):\n",
        "#     planted_year = row['planting_date_reported']\n",
        "#     years = [str(planted_year + i) for i in range(1, 6)]\n",
        "#     losses = [row[year] for year in years if year in df_km.columns]\n",
        "#     return np.nanmean(losses) if losses else np.nan\n",
        "\n",
        "# df_km['loss_pre_5'] = df_km.apply(calculate_loss_pre_5, axis=1)\n",
        "# df_km['loss_post_3'] = df_km.apply(calculate_loss_post_3, axis=1)\n",
        "# df_km['loss_post_5'] = df_km.apply(calculate_loss_post_5, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# df_km[['planting_date_reported', 'loss_pre_5','loss_post_3']].tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-g1u3I6AOfn"
      },
      "outputs": [],
      "source": [
        "# if year not given and have to extract\n",
        "import numpy as np\n",
        "\n",
        "def calculate_loss_pre_5(row):\n",
        "    try:\n",
        "        planted_date = pd.to_datetime(row['planting_date_reported'], errors='coerce')\n",
        "        if pd.isnull(planted_date):\n",
        "            return np.nan\n",
        "        planted_year = planted_date.year\n",
        "        years = [str(planted_year - i) for i in range(1, 6)]\n",
        "        losses = [row[year] for year in years if year in df_km.columns]\n",
        "        return np.nanmean(losses) if losses else np.nan\n",
        "    except Exception as e:\n",
        "        return np.nan\n",
        "\n",
        "def calculate_loss_post_3(row):\n",
        "    try:\n",
        "        planted_date = pd.to_datetime(row['planting_date_reported'], errors='coerce')\n",
        "        if pd.isnull(planted_date):\n",
        "            return np.nan\n",
        "        planted_year = planted_date.year\n",
        "        years = [str(planted_year + i) for i in range(1, 4)]\n",
        "        losses = [row[year] for year in years if year in df_km.columns]\n",
        "        return np.nanmean(losses) if losses else np.nan\n",
        "    except Exception as e:\n",
        "        return np.nan\n",
        "\n",
        "def calculate_loss_post_5(row):\n",
        "    try:\n",
        "        planted_date = pd.to_datetime(row['planting_date_reported'], errors='coerce')\n",
        "        if pd.isnull(planted_date):\n",
        "            return np.nan\n",
        "        planted_year = planted_date.year\n",
        "        years = [str(planted_year + i) for i in range(1, 6)]\n",
        "        losses = [row[year] for year in years if year in df_km.columns]\n",
        "        return np.nanmean(losses) if losses else np.nan\n",
        "    except Exception as e:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "df_km['loss_pre_5'] = df_km.apply(calculate_loss_pre_5, axis=1)\n",
        "df_km['loss_post_3'] = df_km.apply(calculate_loss_post_3, axis=1)\n",
        "df_km['loss_post_5'] = df_km.apply(calculate_loss_post_5, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "un1PPqdmATx_",
        "outputId": "75c811ee-6ed2-411a-f6b6-abcef49d09f2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "0",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8b7895a5-bd85-402a-a18d-1b9046b30460\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>system:index</th>\n",
              "      <th>country</th>\n",
              "      <th>descriptio</th>\n",
              "      <th>planting_d</th>\n",
              "      <th>project_id</th>\n",
              "      <th>site_id_re</th>\n",
              "      <th>site_sqkm</th>\n",
              "      <th>survival_r</th>\n",
              "      <th>trees_plan</th>\n",
              "      <th>.geo</th>\n",
              "      <th>planting_date_reported</th>\n",
              "      <th>loss_pre_5</th>\n",
              "      <th>loss_post_3</th>\n",
              "      <th>loss_post_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>000000000000000000bc</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[38.24315018...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1072</th>\n",
              "      <td>000000000000000000bd</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[38.24283809...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1073</th>\n",
              "      <td>000000000000000000be</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[38.25306721...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1074</th>\n",
              "      <td>000000000000000000bf</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[38.24170103...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>000000000000000000c0</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[38.37896101...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b7895a5-bd85-402a-a18d-1b9046b30460')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b7895a5-bd85-402a-a18d-1b9046b30460 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b7895a5-bd85-402a-a18d-1b9046b30460');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1d893b80-e110-49be-b70e-4172a97e0625\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d893b80-e110-49be-b70e-4172a97e0625')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1d893b80-e110-49be-b70e-4172a97e0625 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "              system:index country  \\\n",
              "1071  000000000000000000bc      TZ   \n",
              "1072  000000000000000000bd      TZ   \n",
              "1073  000000000000000000be      TZ   \n",
              "1074  000000000000000000bf      TZ   \n",
              "1075  000000000000000000c0      TZ   \n",
              "\n",
              "                                             descriptio planting_d  \\\n",
              "1071  ForestNation's mission is to help humanity thr...        NaN   \n",
              "1072  ForestNation's mission is to help humanity thr...        NaN   \n",
              "1073  ForestNation's mission is to help humanity thr...        NaN   \n",
              "1074  ForestNation's mission is to help humanity thr...        NaN   \n",
              "1075  ForestNation's mission is to help humanity thr...        NaN   \n",
              "\n",
              "                         project_id        site_id_re   site_sqkm  survival_r  \\\n",
              "1071  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "1072  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "1073  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "1074  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "1075  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "\n",
              "      trees_plan                                               .geo  \\\n",
              "1071    477975.0  {\"type\":\"Polygon\",\"coordinates\":[[[38.24315018...   \n",
              "1072    477975.0  {\"type\":\"Polygon\",\"coordinates\":[[[38.24283809...   \n",
              "1073    477975.0  {\"type\":\"Polygon\",\"coordinates\":[[[38.25306721...   \n",
              "1074    477975.0  {\"type\":\"Polygon\",\"coordinates\":[[[38.24170103...   \n",
              "1075    477975.0  {\"type\":\"Polygon\",\"coordinates\":[[[38.37896101...   \n",
              "\n",
              "     planting_date_reported  loss_pre_5  loss_post_3  loss_post_5  \n",
              "1071                    NaN         NaN          NaN          NaN  \n",
              "1072                    NaN         NaN          NaN          NaN  \n",
              "1073                    NaN         NaN          NaN          NaN  \n",
              "1074                    NaN         NaN          NaN          NaN  \n",
              "1075                    NaN         NaN          NaN          NaN  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns_to_drop = [str(year) for year in range(2000, 2024)] + ['groups']\n",
        "df_km = df_km.drop(columns=columns_to_drop, errors='ignore')\n",
        "df_km.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OLaN_HmAaX9"
      },
      "outputs": [],
      "source": [
        "df_km.to_csv(\"'../input/Reforestation_Data/plant_forest_loss_by_polygon.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbUqO4WBwLIS"
      },
      "source": [
        "##NDVI per MONTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Sentinel-2 dataset uploading\n",
        "S2 = ee.ImageCollection('COPERNICUS/S2_HARMONIZED')\n",
        "\n",
        "# Calculating NDVI for each Sentinel-2 image\n",
        "def calculate_ndvi(image):\n",
        "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "    return image.addBands(ndvi)\n",
        "\n",
        "# Reducing the image collection to monthly mean NDVI\n",
        "def reduce_to_monthly(image_collection):\n",
        "    monthly = image_collection.select('NDVI').mean().rename('NDVI')\n",
        "    return monthly\n",
        "\n",
        "# Calculating and exporting monthly NDVI for each chunk\n",
        "def calculate_and_export_monthly_ndvi(month):\n",
        "    combined_monthly_data = pd.DataFrame()  \n",
        "\n",
        "    for chunk_index, chunk in enumerate(chunks):\n",
        "        print(f\"Processing chunk {chunk_index + 1}/{len(chunks)} for month {month}...\")\n",
        "\n",
        "        # Converting the current GeoDataFrame chunk to GeoJSON format\n",
        "        gdf_json_chunk = chunk.__geo_interface__\n",
        "        \n",
        "        # Converting the GeoJSON chunk to an Earth Engine FeatureCollection\n",
        "        fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "\n",
        "        # Filtering Sentinel-2 images by the specified month and calculate NDVI\n",
        "        monthly_s2 = S2.filter(ee.Filter.calendarRange(month, month, 'month')).map(calculate_ndvi)\n",
        "        \n",
        "        # Reducing the collection to a single NDVI image for the month\n",
        "        monthly_ndvi = reduce_to_monthly(monthly_s2)\n",
        "\n",
        "        # Calculating the mean NDVI for each feature (polygon) in the chunk\n",
        "        mean_ndvi = monthly_ndvi.reduceRegions(\n",
        "            collection=fc_chunk,\n",
        "            reducer=ee.Reducer.mean(),\n",
        "            scale=30\n",
        "        )\n",
        "\n",
        "        # Exporting the NDVI data to Google Drive\n",
        "        task = ee.batch.Export.table.toDrive(\n",
        "            collection=mean_ndvi,\n",
        "            folder='Reforestation_Data',\n",
        "            description=f'temp_chunk_{chunk_index + 1}_month_{month}',\n",
        "            fileFormat='CSV'\n",
        "        )\n",
        "        task.start()\n",
        "\n",
        "        # Monitoring the export task until it completes\n",
        "        while task.active():\n",
        "            print(f'Waiting for task (id: {task.id}) to complete for chunk {chunk_index + 1}, month {month}...')\n",
        "            time.sleep(30)\n",
        "\n",
        "        print(f'Chunk {chunk_index + 1}, month {month} export done.')\n",
        "\n",
        "        # Loading the exported CSV for the chunk and append it to the combined DataFrame\n",
        "        temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{chunk_index + 1}_month_{month}.csv'\n",
        "        if os.path.exists(temp_chunk_path):\n",
        "            temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "            combined_monthly_data = pd.concat([combined_monthly_data, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "           \n",
        "            os.remove(temp_chunk_path)\n",
        "            print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "    # Saving the combined monthly data to a CSV file\n",
        "    output_monthly_path = f'../input/Reforestation_Data/monthly_combined_ndvi_month_{month}.csv'\n",
        "    combined_monthly_data.to_csv(output_monthly_path, index=False)\n",
        "    print(f\"Combined data for month {month} saved to {output_monthly_path}\")\n",
        "\n",
        "# Looping through each month (1 to 12) to calculate and export NDVI\n",
        "for month in range(1, 13):\n",
        "    calculate_and_export_monthly_ndvi(month)\n",
        "    print(f'Exporting combined NDVI for month {month}...')\n",
        "\n",
        "print('All months processed and combined NDVI files exported.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdKIzRNap5up",
        "outputId": "eee4e647-2f8c-492d-d87a-2430894b8671"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ee/deprecation.py:207: DeprecationWarning: \n",
            "\n",
            "Attention required for COPERNICUS/S2! You are using a deprecated asset.\n",
            "To ensure continued functionality, please update it.\n",
            "Learn more: https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n",
            "\n",
            "  warnings.warn(warning, category=DeprecationWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exporting month 1...\n",
            "Exporting month 2...\n",
            "Exporting month 3...\n",
            "Exporting month 4...\n",
            "Exporting month 5...\n",
            "Exporting month 6...\n",
            "Exporting month 7...\n",
            "Exporting month 8...\n",
            "Exporting month 9...\n",
            "Exporting month 10...\n",
            "Exporting month 11...\n",
            "Exporting month 12...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "Waiting for all tasks to complete...\n",
            "All tasks completed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# # Calculating NDVI\n",
        "# def calculate_ndvi(image):\n",
        "#     ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "#     return image.addBands(ndvi)\n",
        "\n",
        "\n",
        "# def reduce_to_monthly(image_collection):\n",
        "#     monthly = image_collection.select('NDVI').mean().rename('NDVI')\n",
        "#     return monthly\n",
        "\n",
        "# # Processing each month and  combining all chunks\n",
        "# def calculate_and_export_monthly_ndvi(month):\n",
        "#     combined_monthly_data = pd.DataFrame()\n",
        "\n",
        "#     for chunk_index, chunk in enumerate(chunks):\n",
        "#         print(f\"Processing chunk {chunk_index + 1}/{len(chunks)} for month {month}...\")\n",
        "\n",
        "#         gdf_json_chunk = chunk.__geo_interface__\n",
        "#         fc_chunk = geemap.geojson_to_ee(gdf_json_chunk)\n",
        "\n",
        "#         # Filtering Sentinel-2 images by month\n",
        "#         monthly_s2 = S2.filter(ee.Filter.calendarRange(month, month, 'month')).map(calculate_ndvi)\n",
        "#         monthly_ndvi = reduce_to_monthly(monthly_s2)\n",
        "\n",
        "#         # Calculating mean NDVI over each chunk\n",
        "#         mean_ndvi = monthly_ndvi.reduceRegions(\n",
        "#             collection=fc_chunk,\n",
        "#             reducer=ee.Reducer.mean(),\n",
        "#             scale=30\n",
        "#         )\n",
        "\n",
        "\n",
        "#         task = ee.batch.Export.table.toDrive(\n",
        "#             collection=mean_ndvi,\n",
        "#             folder='Reforestation_Data',\n",
        "#             description=f'temp_chunk_{chunk_index + 1}_month_{month}',\n",
        "#             fileFormat='CSV'\n",
        "#         )\n",
        "#         task.start()\n",
        "\n",
        "#         # Just monitoring progress\n",
        "#         while task.active():\n",
        "#             print(f'Waiting for task (id: {task.id}) to complete for chunk {chunk_index + 1}, month {month}...')\n",
        "#             time.sleep(30)\n",
        "\n",
        "#         print(f'Chunk {chunk_index + 1}, month {month} export done.')\n",
        "\n",
        "#         # Loading the exported CSV for the chunks and appending it to the combined DataFrame\n",
        "#         temp_chunk_path = f'../input/Reforestation_Data/temp_chunk_{chunk_index + 1}_month_{month}.csv'\n",
        "#         temp_chunk_df = pd.read_csv(temp_chunk_path)\n",
        "\n",
        "#         combined_monthly_data = pd.concat([combined_monthly_data, temp_chunk_df], ignore_index=True)\n",
        "\n",
        "#     # Saving the  combined monthly data to a CSV file\n",
        "#     output_monthly_path = f'../input/Reforestation_Data/monthly_combined_ndvi_month_{month}.csv'\n",
        "#     combined_monthly_data.to_csv(output_monthly_path, index=False)\n",
        "#     print(f\"Combined data for month {month} saved to {output_monthly_path}\")\n",
        "#             # Removing the temporary chunk file after processing\n",
        "#     if os.path.exists(temp_chunk_path):\n",
        "#         os.remove(temp_chunk_path)\n",
        "#         print(f\"Deleted {temp_chunk_path}\")\n",
        "\n",
        "\n",
        "# for month in range(1, 13):\n",
        "#     calculate_and_export_monthly_ndvi(month)\n",
        "#     print(f'Exporting combined NDVI for month {month}...')\n",
        "\n",
        "# print('All months processed and combined NDVI files exported.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRsLUwaLCaSi",
        "outputId": "198d494c-0ee1-42d5-e0e7-a55eaa53c8c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined data saved to /content/drive/MyDrive/Plant_planet/combined_plant_monthly_ndvi.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "data_dir = f'../input/Reforestation_Data/'\n",
        "\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "for month in range(1, 13):\n",
        "\n",
        "    file_path = os.path.join(data_dir, f'monthly_combined_ndvi_month_{month}.csv')\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "    ndvi_column_name = f'NDVI_0{month}'\n",
        "    df.rename(columns={'mean': ndvi_column_name}, inplace=True)\n",
        "\n",
        "    if combined_df.empty:\n",
        "\n",
        "        combined_df = df\n",
        "    else:\n",
        "\n",
        "        combined_df = pd.merge(combined_df, df, on=list(df.columns.difference([ndvi_column_name])), how='outer')\n",
        "\n",
        "\n",
        "output_file = os.path.join(data_dir, 'combined_plant_monthly_ndvi.csv')\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f'Combined data saved to {output_file}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hPTg5BdCvgJ",
        "outputId": "c2d82bd9-07d9-498f-ca98-c17215789786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1076 entries, 0 to 1075\n",
            "Data columns (total 22 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   system:index  1076 non-null   object \n",
            " 1   country       1076 non-null   object \n",
            " 2   descriptio    1076 non-null   object \n",
            " 3   NDVI_01       1074 non-null   float64\n",
            " 4   planting_d    873 non-null    object \n",
            " 5   project_id    1076 non-null   object \n",
            " 6   site_id_re    1076 non-null   object \n",
            " 7   site_sqkm     1076 non-null   float64\n",
            " 8   survival_r    887 non-null    float64\n",
            " 9   trees_plan    1076 non-null   float64\n",
            " 10  .geo          1076 non-null   object \n",
            " 11  NDVI_02       1074 non-null   float64\n",
            " 12  NDVI_03       1074 non-null   float64\n",
            " 13  NDVI_04       1074 non-null   float64\n",
            " 14  NDVI_05       1074 non-null   float64\n",
            " 15  NDVI_06       1074 non-null   float64\n",
            " 16  NDVI_07       1074 non-null   float64\n",
            " 17  NDVI_08       1074 non-null   float64\n",
            " 18  NDVI_09       1074 non-null   float64\n",
            " 19  NDVI_010      1074 non-null   float64\n",
            " 20  NDVI_011      1074 non-null   float64\n",
            " 21  NDVI_012      1074 non-null   float64\n",
            "dtypes: float64(15), object(7)\n",
            "memory usage: 185.1+ KB\n"
          ]
        }
      ],
      "source": [
        "combined_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "mz2sKt19DJiN",
        "outputId": "ccde54b3-3b35-4dc5-a232-11d05b98b874"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "0",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e65a7f9c-429e-4569-ac4d-9e0d96b69395\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>system:index</th>\n",
              "      <th>country</th>\n",
              "      <th>descriptio</th>\n",
              "      <th>planting_d</th>\n",
              "      <th>project_id</th>\n",
              "      <th>site_id_re</th>\n",
              "      <th>site_sqkm</th>\n",
              "      <th>survival_r</th>\n",
              "      <th>trees_plan</th>\n",
              "      <th>.geo</th>\n",
              "      <th>Top_Three_NDVI_Months</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>000000000000000000bc</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"GeometryCollection\",\"geometries\":[{\"t...</td>\n",
              "      <td>[10, 3, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1072</th>\n",
              "      <td>000000000000000000bd</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[38.24284252...</td>\n",
              "      <td>[10, 1, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1073</th>\n",
              "      <td>000000000000000000be</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[38.25322336...</td>\n",
              "      <td>[10, 2, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1074</th>\n",
              "      <td>000000000000000000bf</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[38.24181244...</td>\n",
              "      <td>[10, 1, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>000000000000000000c0</td>\n",
              "      <td>TZ</td>\n",
              "      <td>ForestNation's mission is to help humanity thr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>proj_6SfzpmoxRIxOlEwzXu6aWQov</td>\n",
              "      <td>reforest_site_72</td>\n",
              "      <td>3694071.01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>477975.0</td>\n",
              "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[38.37923753...</td>\n",
              "      <td>[6, 1, 12]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e65a7f9c-429e-4569-ac4d-9e0d96b69395')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e65a7f9c-429e-4569-ac4d-9e0d96b69395 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e65a7f9c-429e-4569-ac4d-9e0d96b69395');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-47d8e57f-bc93-438f-9c8c-1ec987f6223e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-47d8e57f-bc93-438f-9c8c-1ec987f6223e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-47d8e57f-bc93-438f-9c8c-1ec987f6223e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "              system:index country  \\\n",
              "1071  000000000000000000bc      TZ   \n",
              "1072  000000000000000000bd      TZ   \n",
              "1073  000000000000000000be      TZ   \n",
              "1074  000000000000000000bf      TZ   \n",
              "1075  000000000000000000c0      TZ   \n",
              "\n",
              "                                             descriptio planting_d  \\\n",
              "1071  ForestNation's mission is to help humanity thr...        NaN   \n",
              "1072  ForestNation's mission is to help humanity thr...        NaN   \n",
              "1073  ForestNation's mission is to help humanity thr...        NaN   \n",
              "1074  ForestNation's mission is to help humanity thr...        NaN   \n",
              "1075  ForestNation's mission is to help humanity thr...        NaN   \n",
              "\n",
              "                         project_id        site_id_re   site_sqkm  survival_r  \\\n",
              "1071  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "1072  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "1073  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "1074  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "1075  proj_6SfzpmoxRIxOlEwzXu6aWQov  reforest_site_72  3694071.01       100.0   \n",
              "\n",
              "      trees_plan                                               .geo  \\\n",
              "1071    477975.0  {\"type\":\"GeometryCollection\",\"geometries\":[{\"t...   \n",
              "1072    477975.0  {\"type\":\"Polygon\",\"coordinates\":[[[38.24284252...   \n",
              "1073    477975.0  {\"type\":\"Polygon\",\"coordinates\":[[[38.25322336...   \n",
              "1074    477975.0  {\"type\":\"Polygon\",\"coordinates\":[[[38.24181244...   \n",
              "1075    477975.0  {\"type\":\"Polygon\",\"coordinates\":[[[38.37923753...   \n",
              "\n",
              "     Top_Three_NDVI_Months  \n",
              "1071            [10, 3, 1]  \n",
              "1072            [10, 1, 7]  \n",
              "1073            [10, 2, 3]  \n",
              "1074            [10, 1, 2]  \n",
              "1075            [6, 1, 12]  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=combined_df\n",
        "def find_top_three_ndvi_months(row):\n",
        "    ndvi_columns = ['NDVI_01', 'NDVI_02', 'NDVI_03', 'NDVI_04', 'NDVI_05', 'NDVI_06',\n",
        "                    'NDVI_07', 'NDVI_08', 'NDVI_09', 'NDVI_010', 'NDVI_011', 'NDVI_012']\n",
        "    ndvi_values = []\n",
        "\n",
        "    # Collecting NDVI values and their corresponding months\n",
        "    for column in ndvi_columns:\n",
        "        month_number = column.split('_')[1]\n",
        "        ndvi_value = row[column]\n",
        "        ndvi_values.append((ndvi_value, month_number))\n",
        "\n",
        "    # Sorting the list by NDVI values in descending order\n",
        "    ndvi_values.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # Extracting the top three months\n",
        "    top_three_months = [int(month) for _, month in ndvi_values[:3]]\n",
        "\n",
        "    return top_three_months\n",
        "\n",
        "\n",
        "df['Top_Three_NDVI_Months'] = df.apply(find_top_three_ndvi_months, axis=1)\n",
        "\n",
        "\n",
        "ndvi_columns_to_drop = ['NDVI_01', 'NDVI_02', 'NDVI_03', 'NDVI_04', 'NDVI_05', 'NDVI_06',\n",
        "                        'NDVI_07', 'NDVI_08', 'NDVI_09', 'NDVI_010', 'NDVI_011', 'NDVI_012']\n",
        "\n",
        "\n",
        "df.drop(columns=ndvi_columns_to_drop, inplace=True)\n",
        "\n",
        "\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V6MQ6bIDJV-"
      },
      "outputs": [],
      "source": [
        "df.to_csv('../input/Reforestation_Data/combined_plant_monthly_ndvi.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMFKYrv4FGJv"
      },
      "source": [
        "*** Conclusion ***\n",
        "Manually or through code  combine the unique columns to have complete dataset for each reforestation organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUsqtuQqVsD1",
        "outputId": "626971f5-55e1-4913-cb59-0d8d12972e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged CSV saved to /content/drive/MyDrive/Plant_planet/plant_planet.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "dataframes = []\n",
        "all_columns = set()\n",
        "\n",
        "\n",
        "folder_path = '../input/Reforestation_Data'\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "\n",
        "    if file_name.endswith('.csv') and not file_name.startswith('monthly_combined_ndvi_month_'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        df = pd.read_csv(file_path)\n",
        "        dataframes.append(df)\n",
        "        all_columns.update(df.columns)\n",
        "\n",
        "\n",
        "common_columns = list(set.intersection(*[set(df.columns) for df in dataframes]))\n",
        "\n",
        "\n",
        "merged_df = dataframes[0][common_columns].copy()\n",
        "\n",
        "\n",
        "unique_columns_first_df = list(set(dataframes[0].columns) - set(common_columns))\n",
        "merged_df = pd.concat([merged_df, dataframes[0][unique_columns_first_df]], axis=1)\n",
        "\n",
        "\n",
        "for df in dataframes[1:]:\n",
        "\n",
        "    unique_columns = list(set(df.columns) - set(common_columns))\n",
        "\n",
        "    merged_df = pd.concat([merged_df, df[unique_columns]], axis=1)\n",
        "\n",
        "\n",
        "columns_to_drop = [\"system:index\", \"planting_d\", \"Unnamed: 0\"]\n",
        "merged_df = merged_df.drop(columns=[col for col in columns_to_drop if col in merged_df.columns])\n",
        "\n",
        "\n",
        "if 'site_id_re_x' in merged_df.columns and 'site_id_re_y' in merged_df.columns:\n",
        "    merged_df['site_id_re'] = merged_df['site_id_re_x'].combine_first(merged_df['site_id_re_y'])\n",
        "    merged_df = merged_df.drop(columns=['site_id_re_x', 'site_id_re_y'])\n",
        "\n",
        "# merged_df['host_name'] = 'plant for plant'\n",
        "\n",
        "\n",
        "# merged_df['url'] = 'https://web.plant-for-the-planet.org/en/sirpnigeria-org?site=site_bKgRAzMysTSivm1'\n",
        "\n",
        "output_file_path = os.path.join(folder_path, 'Updated_Reforestation_Data.csv')\n",
        "merged_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Merged CSV saved to {output_file_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
